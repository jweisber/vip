<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="20 Lindley’s Paradox | Odds &amp; Ends" />
<meta property="og:type" content="book" />

<meta property="og:image" content="img/social_image.png" />
<meta property="og:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="github-repo" content="jweisber/vip" />

<meta name="author" content="Jonathan Weisberg" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An open access textbook for introductory philosophy courses on probability and inductive logic.">

<title>20 Lindley’s Paradox | Odds &amp; Ends</title>

<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@jweisber" />
<meta name="twitter:creator" content="@jweisber" />
<meta name="twitter:title" content="20 Lindley’s Paradox | Odds &amp; Ends" />
<meta name="twitter:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="twitter:image" content="img/social_image.png" />


<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["output/HTML-CSS"],
  "HTML-CSS": {
    availableFonts: ["Gyre-Pagella"],
    preferredFont: "Gyre-Pagella",
    webFont: "Gyre-Pagella",
    imageFont: "Gyre-Pagella"
  }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      extensions: ["color.js"]
    }
  });
</script>




<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="custom.css" type="text/css" />

</head>

<body>


<div style="display: none;">
$$
  \newcommand{\given}{\mid}
  \renewcommand{\neg}{\mathbin{\sim}}
  \renewcommand{\wedge}{\mathbin{\&}}
  \newcommand{\p}{Pr}
  \newcommand{\deg}{^{\circ}}
  \newcommand{\E}{E}
  \newcommand{\EU}{EU}
  \newcommand{\u}{U}
  \newcommand{\pr}{Pr}
  \newcommand{\po}{Pr^*}
  \definecolor{bookred}{RGB}{228,6,19}
  \definecolor{bookblue}{RGB}{0,92,169}
  \definecolor{bookpurple}{RGB}{114,49,94} 
$$
</div>

<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li class="part"><span><b>Part I</b></span></li>
<li class="has-sub"><a href="the-monty-hall-problem.html#the-monty-hall-problem"><span class="toc-section-number">1</span> The Monty Hall Problem</a><ul>
<li><a href="the-monty-hall-problem.html#diagramming-the-solution"><span class="toc-section-number">1.1</span> Diagramming the Solution</a></li>
<li><a href="the-monty-hall-problem.html#lessons"><span class="toc-section-number">1.2</span> Lessons Learned</a></li>
<li><a href="the-monty-hall-problem.html#exercises">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="logic.html#logic"><span class="toc-section-number">2</span> Logic</a><ul>
<li><a href="logic.html#validity-soundness"><span class="toc-section-number">2.1</span> Validity &amp; Soundness</a></li>
<li><a href="logic.html#propositions"><span class="toc-section-number">2.2</span> Propositions</a></li>
<li><a href="logic.html#visualizing-propositions"><span class="toc-section-number">2.3</span> Visualizing Propositions</a></li>
<li><a href="logic.html#strength"><span class="toc-section-number">2.4</span> Strength</a></li>
<li><a href="logic.html#indargs"><span class="toc-section-number">2.5</span> Forms of Inductive Argument</a></li>
<li><a href="logic.html#exercises-1">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="truth-tables.html#truth-tables"><span class="toc-section-number">3</span> Truth Tables</a><ul>
<li><a href="truth-tables.html#connectives"><span class="toc-section-number">3.1</span> Connectives</a></li>
<li><a href="truth-tables.html#truth-tables-1"><span class="toc-section-number">3.2</span> Truth Tables</a></li>
<li><a href="truth-tables.html#logical-truths-contradictions"><span class="toc-section-number">3.3</span> Logical Truths &amp; Contradictions</a></li>
<li><a href="truth-tables.html#mutual-exclusivity-truth-tables"><span class="toc-section-number">3.4</span> Mutual Exclusivity &amp; Truth Tables</a></li>
<li><a href="truth-tables.html#entailment-equivalence"><span class="toc-section-number">3.5</span> Entailment &amp; Equivalence</a></li>
<li><a href="truth-tables.html#summary"><span class="toc-section-number">3.6</span> Summary</a></li>
<li><a href="truth-tables.html#exercises-2">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="the-gamblers-fallacy.html#the-gamblers-fallacy"><span class="toc-section-number">4</span> The Gambler’s Fallacy</a><ul>
<li><a href="the-gamblers-fallacy.html#independence"><span class="toc-section-number">4.1</span> Independence</a></li>
<li><a href="the-gamblers-fallacy.html#fairness"><span class="toc-section-number">4.2</span> Fairness</a></li>
<li><a href="the-gamblers-fallacy.html#the-gamblers-fallacy-1"><span class="toc-section-number">4.3</span> The Gambler’s Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#ignorance-is-not-a-fallacy"><span class="toc-section-number">4.4</span> Ignorance Is Not a Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#the-hot-hand-fallacy"><span class="toc-section-number">4.5</span> The Hot Hand Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#exercises-3">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="calculating-probabilities.html#calculating-probabilities"><span class="toc-section-number">5</span> Calculating Probabilities</a><ul>
<li><a href="calculating-probabilities.html#multiplying-probabilities"><span class="toc-section-number">5.1</span> Multiplying Probabilities</a></li>
<li><a href="calculating-probabilities.html#adding-probabilities"><span class="toc-section-number">5.2</span> Adding Probabilities</a></li>
<li><a href="calculating-probabilities.html#exclusivity-vs.independence"><span class="toc-section-number">5.3</span> Exclusivity vs. Independence</a></li>
<li><a href="calculating-probabilities.html#tautologies-contradictions-and-equivalent-propositions"><span class="toc-section-number">5.4</span> Tautologies, Contradictions, and Equivalent Propositions</a></li>
<li><a href="calculating-probabilities.html#the-language-of-events"><span class="toc-section-number">5.5</span> The Language of Events</a></li>
<li><a href="calculating-probabilities.html#summary-1"><span class="toc-section-number">5.6</span> Summary</a></li>
<li><a href="calculating-probabilities.html#exercises-4">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="conditional-probability.html#conditional-probability"><span class="toc-section-number">6</span> Conditional Probability</a><ul>
<li><a href="conditional-probability.html#calculating-conditional-probability"><span class="toc-section-number">6.1</span> Calculating Conditional Probability</a></li>
<li><a href="conditional-probability.html#conditional-probability-trees"><span class="toc-section-number">6.2</span> Conditional Probability &amp; Trees</a></li>
<li><a href="conditional-probability.html#more-examples"><span class="toc-section-number">6.3</span> More Examples</a></li>
<li><a href="conditional-probability.html#order-matters"><span class="toc-section-number">6.4</span> Order Matters</a></li>
<li><a href="conditional-probability.html#declaring-independence"><span class="toc-section-number">6.5</span> Declaring Independence</a></li>
<li><a href="conditional-probability.html#ch6ex">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="calculating-probabilities-part-ii.html#calculating-probabilities-part-ii"><span class="toc-section-number">7</span> Calculating Probabilities, Part II</a><ul>
<li><a href="calculating-probabilities-part-ii.html#the-negation-rule"><span class="toc-section-number">7.1</span> The Negation Rule</a></li>
<li><a href="calculating-probabilities-part-ii.html#the-general-addition-rule"><span class="toc-section-number">7.2</span> The General Addition Rule</a></li>
<li><a href="calculating-probabilities-part-ii.html#the-general-multiplication-rule"><span class="toc-section-number">7.3</span> The General Multiplication Rule</a></li>
<li><a href="calculating-probabilities-part-ii.html#laplaces-urn-puzzle"><span class="toc-section-number">7.4</span> Laplace’s Urn Puzzle</a></li>
<li><a href="calculating-probabilities-part-ii.html#the-law-of-total-probability"><span class="toc-section-number">7.5</span> The Law of Total Probability</a></li>
<li><a href="calculating-probabilities-part-ii.html#example"><span class="toc-section-number">7.6</span> Example</a></li>
<li><a href="calculating-probabilities-part-ii.html#exercises-5">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="chbayes.html#chbayes"><span class="toc-section-number">8</span> Bayes’ Theorem</a><ul>
<li><a href="chbayes.html#bayes-theorem"><span class="toc-section-number">8.1</span> Bayes’ Theorem</a></li>
<li><a href="chbayes.html#understanding-bayes-theorem"><span class="toc-section-number">8.2</span> Understanding Bayes’ Theorem</a></li>
<li><a href="chbayes.html#bayes-long-theorem"><span class="toc-section-number">8.3</span> Bayes’ Long Theorem</a></li>
<li><a href="chbayes.html#example-1"><span class="toc-section-number">8.4</span> Example</a></li>
<li><a href="chbayes.html#baserate"><span class="toc-section-number">8.5</span> The Base Rate Fallacy</a></li>
<li><a href="chbayes.html#exercises-6">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="multiple-conditions.html#multiple-conditions"><span class="toc-section-number">9</span> Multiple Conditions</a><ul>
<li><a href="multiple-conditions.html#multiple-draws"><span class="toc-section-number">9.1</span> Multiple Draws</a></li>
<li><a href="multiple-conditions.html#multiple-witnesses"><span class="toc-section-number">9.2</span> Multiple Witnesses</a></li>
<li><a href="multiple-conditions.html#without-replacement"><span class="toc-section-number">9.3</span> Without Replacement</a></li>
<li><a href="multiple-conditions.html#multiplying-conditional-probabilities"><span class="toc-section-number">9.4</span> Multiplying Conditional Probabilities</a></li>
<li><a href="multiple-conditions.html#summary-2"><span class="toc-section-number">9.5</span> Summary</a></li>
<li><a href="multiple-conditions.html#exercises-7">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="probability-induction.html#probability-induction"><span class="toc-section-number">10</span> Probability &amp; Induction</a><ul>
<li><a href="probability-induction.html#generalizing-from-observed-instances"><span class="toc-section-number">10.1</span> Generalizing from Observed Instances</a></li>
<li><a href="probability-induction.html#real-life-is-more-complicated"><span class="toc-section-number">10.2</span> Real Life Is More Complicated</a></li>
<li><a href="probability-induction.html#bayesibe"><span class="toc-section-number">10.3</span> Inference to the Best Explanation</a></li>
</ul></li>
<li class="part"><span><b>Part II</b></span></li>
<li class="has-sub"><a href="expected-value.html#expected-value"><span class="toc-section-number">11</span> Expected Value</a><ul>
<li><a href="expected-value.html#expected-monetary-values"><span class="toc-section-number">11.1</span> Expected Monetary Values</a></li>
<li><a href="expected-value.html#visualizing-expectations"><span class="toc-section-number">11.2</span> Visualizing Expectations</a></li>
<li><a href="expected-value.html#more-than-two-outcomes"><span class="toc-section-number">11.3</span> More Than Two Outcomes</a></li>
<li><a href="expected-value.html#fair-prices"><span class="toc-section-number">11.4</span> Fair Prices</a></li>
<li><a href="expected-value.html#other-goods"><span class="toc-section-number">11.5</span> Other Goods</a></li>
<li><a href="expected-value.html#decision-tables"><span class="toc-section-number">11.6</span> Decision Tables</a></li>
<li><a href="expected-value.html#exercises-8">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="utility.html#utility"><span class="toc-section-number">12</span> Utility</a><ul>
<li><a href="utility.html#subjectivity-objectivity"><span class="toc-section-number">12.1</span> Subjectivity &amp; Objectivity</a></li>
<li><a href="utility.html#the-general-recipe"><span class="toc-section-number">12.2</span> The General Recipe</a></li>
<li><a href="utility.html#choosing-scales"><span class="toc-section-number">12.3</span> Choosing Scales</a></li>
<li><a href="utility.html#a-limitation-the-expected-utility-assumption"><span class="toc-section-number">12.4</span> A Limitation: The Expected Utility Assumption</a></li>
<li><a href="utility.html#the-value-of-money"><span class="toc-section-number">12.5</span> The Value of Money</a></li>
<li><a href="utility.html#exercises-9">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="challenges-to-expected-utility.html#challenges-to-expected-utility"><span class="toc-section-number">13</span> Challenges to Expected Utility</a><ul>
<li><a href="challenges-to-expected-utility.html#the-allais-paradox"><span class="toc-section-number">13.1</span> The Allais Paradox</a></li>
<li><a href="challenges-to-expected-utility.html#the-sure-thing-principle"><span class="toc-section-number">13.2</span> The Sure-thing Principle</a></li>
<li><a href="challenges-to-expected-utility.html#prescriptive-vs.descriptive"><span class="toc-section-number">13.3</span> Prescriptive vs. Descriptive</a></li>
<li><a href="challenges-to-expected-utility.html#the-ellsberg-paradox"><span class="toc-section-number">13.4</span> The Ellsberg Paradox</a></li>
<li><a href="challenges-to-expected-utility.html#ellsberg-allais"><span class="toc-section-number">13.5</span> Ellsberg &amp; Allais</a></li>
<li><a href="challenges-to-expected-utility.html#exercises-10">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="infinity-beyond.html#infinity-beyond"><span class="toc-section-number">14</span> Infinity &amp; Beyond</a><ul>
<li><a href="infinity-beyond.html#the-st.petersburg-paradox"><span class="toc-section-number">14.1</span> The St. Petersburg Paradox</a></li>
<li><a href="infinity-beyond.html#bernoullis-solution"><span class="toc-section-number">14.2</span> Bernoulli’s Solution</a></li>
<li><a href="infinity-beyond.html#st.petersburgs-revenge"><span class="toc-section-number">14.3</span> St. Petersburg’s Revenge</a></li>
<li><a href="infinity-beyond.html#pascals-wager"><span class="toc-section-number">14.4</span> Pascal’s Wager</a></li>
<li><a href="infinity-beyond.html#responses-to-pascals-wager"><span class="toc-section-number">14.5</span> Responses to Pascal’s Wager</a></li>
<li><a href="infinity-beyond.html#exercises-11">Exercises</a></li>
</ul></li>
<li class="part"><span><b>Part III</b></span></li>
<li class="has-sub"><a href="two-schools.html#two-schools"><span class="toc-section-number">15</span> Two Schools</a><ul>
<li><a href="two-schools.html#probability-as-frequency"><span class="toc-section-number">15.1</span> Probability as Frequency</a></li>
<li><a href="two-schools.html#probability-as-belief"><span class="toc-section-number">15.2</span> Probability as Belief</a></li>
<li><a href="two-schools.html#which-kind-of-probability"><span class="toc-section-number">15.3</span> Which Kind of Probability?</a></li>
<li><a href="two-schools.html#frequentism"><span class="toc-section-number">15.4</span> Frequentism</a></li>
<li><a href="two-schools.html#bayesianism"><span class="toc-section-number">15.5</span> Bayesianism</a></li>
</ul></li>
<li class="has-sub"><a href="beliefs-betting-rates.html#beliefs-betting-rates"><span class="toc-section-number">16</span> Beliefs &amp; Betting Rates</a><ul>
<li><a href="beliefs-betting-rates.html#measuring-personal-probabilities"><span class="toc-section-number">16.1</span> Measuring Personal Probabilities</a></li>
<li><a href="beliefs-betting-rates.html#things-to-watch-out-for"><span class="toc-section-number">16.2</span> Things to Watch Out For</a></li>
<li><a href="beliefs-betting-rates.html#indirect-measurements"><span class="toc-section-number">16.3</span> Indirect Measurements</a></li>
<li><a href="beliefs-betting-rates.html#exercises-12">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="dutch-books.html#dutch-books"><span class="toc-section-number">17</span> Dutch Books</a><ul>
<li><a href="dutch-books.html#dutch-books-1"><span class="toc-section-number">17.1</span> Dutch Books</a></li>
<li><a href="dutch-books.html#bankteller"><span class="toc-section-number">17.2</span> The Bankteller Fallacy</a></li>
<li><a href="dutch-books.html#dutch-books-in-general"><span class="toc-section-number">17.3</span> Dutch Books in General</a></li>
<li><a href="dutch-books.html#exercises-13">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="priors.html#priors"><span class="toc-section-number">18</span> The Problem of Priors</a><ul>
<li><a href="priors.html#priors-posteriors"><span class="toc-section-number">18.1</span> Priors &amp; Posteriors</a></li>
<li><a href="priors.html#the-principle-of-indifference"><span class="toc-section-number">18.2</span> The Principle of Indifference</a></li>
<li><a href="priors.html#the-continuous-principle-of-indifference"><span class="toc-section-number">18.3</span> The Continuous Principle of Indifference</a></li>
<li><a href="priors.html#bertrands-paradox"><span class="toc-section-number">18.4</span> Bertrand’s Paradox</a></li>
<li><a href="priors.html#the-problem-of-priors"><span class="toc-section-number">18.5</span> The Problem of Priors</a></li>
<li><a href="priors.html#exercises-14">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="significance-testing.html#significance-testing"><span class="toc-section-number">19</span> Significance Testing</a><ul>
<li><a href="significance-testing.html#coincidence"><span class="toc-section-number">19.1</span> Coincidence</a></li>
<li><a href="significance-testing.html#making-it-precise"><span class="toc-section-number">19.2</span> Making it Precise</a></li>
<li><a href="significance-testing.html#levels-of-significance"><span class="toc-section-number">19.3</span> Levels of Significance</a></li>
<li><a href="significance-testing.html#normal-approximation"><span class="toc-section-number">19.4</span> Normal Approximation</a></li>
<li><a href="significance-testing.html#the-68-95-99-rule"><span class="toc-section-number">19.5</span> The 68-95-99 Rule</a></li>
<li><a href="significance-testing.html#binomial-probabilities"><span class="toc-section-number">19.6</span> Binomial Probabilities</a></li>
<li><a href="significance-testing.html#significance-testing-1"><span class="toc-section-number">19.7</span> Significance Testing</a></li>
<li><a href="significance-testing.html#warnings"><span class="toc-section-number">19.8</span> Warnings</a></li>
<li><a href="significance-testing.html#exercises-15">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="chlindley.html#chlindley"><span class="toc-section-number">20</span> Lindley’s Paradox</a><ul>
<li><a href="chlindley.html#significance-subjectivity"><span class="toc-section-number">20.1</span> Significance &amp; Subjectivity</a></li>
<li><a href="chlindley.html#making-it-concrete"><span class="toc-section-number">20.2</span> Making It Concrete</a></li>
<li><a href="chlindley.html#the-role-of-priors-in-significance-testing"><span class="toc-section-number">20.3</span> The Role of Priors in Significance Testing</a></li>
<li><a href="chlindley.html#lindleys-paradox"><span class="toc-section-number">20.4</span> Lindley’s Paradox</a></li>
<li><a href="chlindley.html#a-bayesian-analysis"><span class="toc-section-number">20.5</span> A Bayesian Analysis</a></li>
<li><a href="chlindley.html#exercises-16">Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="has-sub"><a href="cheat-sheet.html#cheat-sheet"><span class="toc-section-number">A</span> Cheat Sheet</a><ul>
<li><a href="cheat-sheet.html#deductive-logic">Deductive Logic</a></li>
<li><a href="cheat-sheet.html#probability">Probability</a></li>
<li><a href="cheat-sheet.html#decision-theory">Decision Theory</a></li>
<li><a href="cheat-sheet.html#bayesianism-1">Bayesianism</a></li>
<li><a href="cheat-sheet.html#frequentism-1">Frequentism</a></li>
</ul></li>
<li class="has-sub"><a href="the-axioms-of-probability.html#the-axioms-of-probability"><span class="toc-section-number">B</span> The Axioms of Probability</a><ul>
<li><a href="the-axioms-of-probability.html#theories-and-axioms">Theories and Axioms</a></li>
<li><a href="the-axioms-of-probability.html#the-three-axioms-of-probability">The Three Axioms of Probability</a></li>
<li><a href="the-axioms-of-probability.html#first-steps">First Steps</a></li>
<li><a href="the-axioms-of-probability.html#conditional-probability-the-multiplication-rule">Conditional Probability &amp; the Multiplication Rule</a></li>
<li><a href="the-axioms-of-probability.html#equivalence-general-addition">Equivalence &amp; General Addition</a></li>
<li><a href="the-axioms-of-probability.html#total-probability-bayes-theorem">Total Probability &amp; Bayes’ Theorem</a></li>
<li><a href="the-axioms-of-probability.html#independence-1">Independence</a></li>
</ul></li>
<li class="has-sub"><a href="grue.html#grue"><span class="toc-section-number">C</span> The Grue Paradox</a><ul>
<li><a href="grue.html#a-gruesome-concept">A Gruesome Concept</a></li>
<li><a href="grue.html#the-paradox">The Paradox</a></li>
<li><a href="grue.html#grue-artificial-intelligence">Grue &amp; Artificial Intelligence</a></li>
<li><a href="grue.html#disjunctivitis">Disjunctivitis</a></li>
<li><a href="grue.html#time-dependence">Time Dependence</a></li>
<li><a href="grue.html#the-moral">The Moral</a></li>
</ul></li>
<li class="has-sub"><a href="the-problem-of-induction.html#the-problem-of-induction"><span class="toc-section-number">D</span> The Problem of Induction</a><ul>
<li><a href="the-problem-of-induction.html#the-dilemma">The Dilemma</a></li>
<li><a href="the-problem-of-induction.html#the-problem-of-induction-vs.the-grue-paradox">The Problem of Induction vs. the Grue Paradox</a></li>
<li><a href="the-problem-of-induction.html#probability-theory-to-the-rescue">Probability Theory to the Rescue?</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="chlindley" class="section level1">
<h1><span class="header-section-number">20</span> Lindley’s Paradox</h1>
<div class="epigraph">
<p>
Inside every non-Bayesian, there is a Bayesian struggling to get out.<br />
—Dennis V. Lindley (quoted by E. T. Jaynes)
</p>
</div>
<p><span class="newthought">Significance</span> testing is all about whether the outcome would be too much of a coincidence for the hypothesis to be true. But how much of a coincidence is too much? Should we reject a hypothesis when we find results that are significant at the <span class="math inline">\(.05\)</span>-level? At the <span class="math inline">\(.01\)</span>-level?</p>
<div id="significance-subjectivity" class="section level2">
<h2><span class="header-section-number">20.1</span> Significance &amp; Subjectivity</h2>
<p><span class="newthought">Critics</span> say this decision is subjective. In the social sciences, researchers have mainly adopted <span class="math inline">\(.05\)</span> as the cutoff, while in other sciences the convention is to use <span class="math inline">\(.01\)</span>. But we saw there’s nothing special about these numbers. Except that they’re easy to estimate without the help of a computer.</p>
<p>It makes a big difference what cutoff we choose, though. The lower the cutoff, the less often a result will be deemed significant. So a lower cutoff means fewer hypotheses ruled out, which means fewer scientific discoveries.</p>
<p>Think of all the studies of potential cancer treatments being done around the world. In each study, the researcher is testing the hypothesis that their treatment has no effect, in the hopes of disproving that hypothesis. They’re hoping enough patients get better in their study to show the treatment genuinely helps. If just a few more patients improve with their treatment compared to a placebo, it could just be a fluke. But if a lot more improve, that means the treatment is probably making a real difference.</p>
<p>The lower the significance level, the more patients have to improve before they can call their study’s results “significant”. So a lower significance level means fewer treatments will be approved by the medical community.</p>
<p>That might seem like a bad thing, but it also has an upside. It means fewer <em>bogus</em> treatments being adopted.</p>
<p><label for="tufte-mn-36" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-36" class="margin-toggle"><span class="marginnote"><span style="display: block;">Medical treatments are often expensive, painful, or dangerous. So it’s a serious problem to approve ones that don’t actually help.</span>
<span style="display: block;">But sometimes it just leads to <a href="https://xkcd.com/882/">silly dietary choices</a>.</span></span></p>
<p>After all, occasionally a study of a useless treatment will have lots of patients improving anyway, just by luck. When that happens, the medical community adopts a treatment that doesn’t actually work. And there might be lots of studies out there experimenting with treatments that don’t actually help. So if we do enough studies, we’re bound to get flukey results in some of them, and adopt a treatment that doesn’t actually work by mistake.</p>
</div>
<div id="making-it-concrete" class="section level2">
<h2><span class="header-section-number">20.2</span> Making It Concrete</h2>
<p><span class="newthought">Suppose</span> for example there are only two types of potential treatment being studied. The useless kind just leave the patient’s recovery up to chance: it’s a <span class="math inline">\(50\%\)</span> shot. But the effective kind increase their chances of recovery by <span class="math inline">\(15\%\)</span>: they have a <span class="math inline">\(65\%\)</span> chance of getting better with these treatments.</p>
<p>Let’s also imagine we’re studying <span class="math inline">\(200\)</span> different treatments, <span class="math inline">\(160\)</span> of which are actually useless and <span class="math inline">\(40\)</span> of which are effective. Since we don’t know which are which, we’ll run <span class="math inline">\(200\)</span> studies, one for each treatment. And just to make things concrete, let’s suppose each study has <span class="math inline">\(n = 100\)</span> subjects enrolled.</p>
<p>What will happen if our researchers set the cutoff for statistical significance at <span class="math inline">\(.05\)</span>? Only a few of the bogus treatments will be approved. Just <span class="math inline">\(5\%\)</span> of those studies will get flukey, statistically significant results. And half of those will look like they’re harming patients’ chances of recovery, rather than helping. So only <span class="math inline">\(2.5\%\)</span> of the <span class="math inline">\(160\)</span> bogus treatments will be approved, which is <span class="math inline">\(4\)</span> treatments.</p>
<p>But also, a good number of the genuine treatments will be missed. Only about <span class="math inline">\(85\%\)</span> will be discovered as it turns out: see Figure <a href="chlindley.html#fig:hundredstudies">20.1</a>.</p>
<div class="figure"><span id="fig:hundredstudies"></span>
<p class="caption marginnote shownote">
Figure 20.1: A significance cutoff of <span class="math inline">\(.05\)</span>, when running studies with <span class="math inline">\(100\)</span> patients in each. The red curve represents the probable outcomes when studying useless treatments (<span class="math inline">\(50\%\)</span> chance of recovery), the green curve represents the probable outcomes when studying effective treatments (<span class="math inline">\(65\%\)</span> chance of recovery).
</p>
<img src="_main_files/figure-html/hundredstudies-1.png" alt="A significance cutoff of $.05$, when running studies with $100$ patients in each. The red curve represents the probable outcomes when studying useless treatments ($50\%$ chance of recovery), the green curve represents the probable outcomes when studying effective treatments ($65\%$ chance of recovery)." width="672"  />
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:hundredstudiesgrid"></span>
<img src="_main_files/figure-html/hundredstudiesgrid-1.png" alt="The results of our $200$ studies. Green pills represent genuinely effective treatments, red pills represent useless treatments. The dashed green line represents the treatments we approve: only $34$ out of $38$ of these are genuinely effective." width="672"  />
<!--
<p class="caption marginnote">-->Figure 20.2: The results of our <span class="math inline">\(200\)</span> studies. Green pills represent genuinely effective treatments, red pills represent useless treatments. The dashed green line represents the treatments we approve: only <span class="math inline">\(34\)</span> out of <span class="math inline">\(38\)</span> of these are genuinely effective.<!--</p>-->
<!--</div>--></span>
</p>
<p>As a result, only about <span class="math inline">\(89\%\)</span> of the treatments we approve will actually be genuinely effective. We can’t see this from Figure <a href="chlindley.html#fig:hundredstudies">20.1</a> because it doesn’t show the base rates. We have to turn to the kind of reasoning we did in the taxicab problem instead.</p>
<p>Figure <a href="chlindley.html#fig:hundredstudiesgrid">20.2</a> shows the results. Since <span class="math inline">\(2.5\%\)</span> of the <span class="math inline">\(160\)</span> useless treatments (red pills) will be approved, that’s <span class="math inline">\(4\)</span> bogus “discoveries”. And since <span class="math inline">\(85\%\)</span> of the <span class="math inline">\(40\)</span> genuine ones (green pills) will be approved, that’s about <span class="math inline">\(34\)</span> genuine treatments discovered. So only about <span class="math inline">\(34/38 \approx 89\%\)</span> of our approved treatments actually work.</p>
<p>We could improve this percentage by lowering the threshold for significance to <span class="math inline">\(.01\)</span>. But then only half of the genuine treatments would be identified by our studies (Figure <a href="chlindley.html#fig:hundredstudies2">20.3</a>).</p>
<div class="figure"><span id="fig:hundredstudies2"></span>
<p class="caption marginnote shownote">
Figure 20.3: A significance cutoff of <span class="math inline">\(.01\)</span> when running studies with <span class="math inline">\(100\)</span> patients in each. The red curve represents the probable outcomes when studying useless treatments (<span class="math inline">\(50\%\)</span> chance of recovery), the green curve represents the probable outcomes when studying effective treatments (<span class="math inline">\(65\%\)</span> chance of recovery).
</p>
<img src="_main_files/figure-html/hundredstudies2-1.png" alt="A significance cutoff of $.01$ when running studies with $100$ patients in each. The red curve represents the probable outcomes when studying useless treatments ($50\%$ chance of recovery), the green curve represents the probable outcomes when studying effective treatments ($65\%$ chance of recovery)." width="672"  />
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:hundredstudiesgrid2"></span>
<img src="_main_files/figure-html/hundredstudiesgrid2-1.png" alt="With a stricter significance cutoff of $.01$, we make fewer &quot;bogus&quot; discoveries. But we miss out on a lot of genuine discoveries too." width="672"  />
<!--
<p class="caption marginnote">-->Figure 20.4: With a stricter significance cutoff of <span class="math inline">\(.01\)</span>, we make fewer “bogus” discoveries. But we miss out on a lot of genuine discoveries too.<!--</p>-->
<!--</div>--></span>
</p>
<p>Figure <a href="chlindley.html#fig:hundredstudiesgrid2">20.4</a> shows what happens now: about <span class="math inline">\(95\%\)</span> of our approved treatments will be genuine. We discover half of the <span class="math inline">\(40\)</span> genuine treatments, which is <span class="math inline">\(20\)</span>. And <span class="math inline">\(.005\)</span> of the <span class="math inline">\(160\)</span> useless treatments is <span class="math inline">\(.8\)</span>, which we’ll round up to <span class="math inline">\(1\)</span> for purposes of the diagram. So <span class="math inline">\(20/21 \approx .95\)</span> of our approved treatments are genuinely effective.</p>
<p>But we’ve paid a dear price for this increase in precision: we’ve failed to identify half the treatments there are to be discovered. We’ve missed out on a lot of potential benefit to our patients.</p>
<p>Notice, however, that if bogus treatments were rarer the problem wouldn’t be so pressing. For example, Figure <a href="chlindley.html#fig:hundredstudiesgrid3">20.5</a> shows what would happen if half the treatments being studied were bogus, instead of <span class="math inline">\(80\%\)</span>. Then we could have stuck with the <span class="math inline">\(.05\)</span> threshold and still had very good results: we would discover about <span class="math inline">\(85\)</span> genuine treatments and only approved about <span class="math inline">\(3\)</span> bogus ones, a precision of about <span class="math inline">\(97\%\)</span>.</p>
<p>There are two lessons here. First, lowering the threshold for significance has both benefits and costs. A lower threshold means fewer false discoveries, but it means fewer genuine discoveries too. Second, the base rate informs our decision about how to make this tradeoff. The more false hypotheses there are to watch out for, the stronger the incentive to use a lower threshold.<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> Is the real base rate low enough that medical researchers actually need to worry? Yes, according to <a href="https://www.doi.org/10.1038/nrd3439-c1">this research</a>.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:hundredstudiesgrid3"></span>
<img src="_main_files/figure-html/hundredstudiesgrid3-1.png" alt="A significance cutoff of $.05$ does much better if the base rate is more favourable." width="672"  />
<!--
<p class="caption marginnote">-->Figure 20.5: A significance cutoff of <span class="math inline">\(.05\)</span> does much better if the base rate is more favourable.<!--</p>-->
<!--</div>--></span>
</p>
</div>
<div id="the-role-of-priors-in-significance-testing" class="section level2">
<h2><span class="header-section-number">20.3</span> The Role of Priors in Significance Testing</h2>
<p><span class="newthought">Bayesian</span> critics of significance testing conclude that, when we choose where to set the cutoff, we’re choosing based on our “priors”. We’re relying on assumptions about the base rate, the prior probability of the null hypothesis.</p>
<p>So, these critics say, frequentism faces exactly the same subjectivity problem as Bayesianism. Bayesians use Bayes’ theorem to evaluate hypothesis <span class="math inline">\(H\)</span> in light of evidence <span class="math inline">\(E\)</span>:
<span class="math display">\[ \p(H \given E) = \p(H) \frac{\p(E \given H)}{\p(E)} .\]</span>
But we saw there’s no recipe for calculating the prior probability of <span class="math inline">\(H\)</span>, <span class="math inline">\(\p(H)\)</span>. You just have to start with your best guess about how plausible <span class="math inline">\(H\)</span> is. Likewise, frequentists have to start with their best guess about how many of the potential cancer treatments being studied are bogus, and how many are real. That’s how we decide where to set the cutoff for statistical significance.</p>
<p>From a Bayesian point of view, significance testing focuses on just one term in Bayes’ theorem, the numerator <span class="math inline">\(\p(E \given H)\)</span>. We suppose the hypothesis is true, and then consider how likely the sort of outcome we’ve observed is. But Bayes’ theorem tells us we need more information to find what we really want, namely <span class="math inline">\(\p(H \given E)\)</span>. And for that, we need to know <span class="math inline">\(\p(H)\)</span>, how plausible <span class="math inline">\(H\)</span> is to begin with.</p>
<p>So significance testing is based on a mistake, according to Bayesian critics. It ignores essential <a href="chbayes.html#baserate">base rate</a> information, namely how many of the hypotheses we study are true. And this can lead to very wrong results, as we learned from the taxicab problem in <a href="chbayes.html#chbayes">Chapter</a> <a href="chbayes.html#chbayes">8</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:unnamed-chunk-154"></span>
<img src="img/jeffreys.png" alt="Harold Jeffreys (1891--1989) first raised the problem known as Lindley's paradox in $1939$. Dennis Lindley labeled it a paradox in $1957$, hence the name. It is sometimes called the Jeffreys-Lindley paradox." width="87"  />
<!--
<p class="caption marginnote">-->Figure 20.6: Harold Jeffreys (1891–1989) first raised the problem known as Lindley’s paradox in <span class="math inline">\(1939\)</span>. Dennis Lindley labeled it a paradox in <span class="math inline">\(1957\)</span>, hence the name. It is sometimes called the Jeffreys-Lindley paradox.<!--</p>-->
<!--</div>--></span>
</p>
<p>This critique of frequentism is sharpened by a famous problem known as Lindley’s paradox.</p>
</div>
<div id="lindleys-paradox" class="section level2">
<h2><span class="header-section-number">20.4</span> Lindley’s Paradox</h2>
<p><label for="tufte-mn-37" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-37" class="margin-toggle"><span class="marginnote"><span style="display: block;">The tulip example is based on an example from Henry Kyburg’s book <em>Logical Foundations of Statistical Inference</em>. It also appears in Howson &amp; Urbach’s <em>Scientific Reasoning: A Bayesian Approach</em>.</span></span></p>
<p><span class="newthought">Suppose</span> a florist receives a large shipment of tulip bulbs with the label scratched off. The company that sent the shipment only sends two kinds of shipments. The first kind contains <span class="math inline">\(25\%\)</span> red bulbs, the second kind has <span class="math inline">\(50\%\)</span> red bulbs.</p>
<p>The two kinds of shipment are equally common. So the store owner figures this shipment could be of either kind, with equal probability.</p>
<p>To figure out which kind of shipment she has, she takes a sample of <span class="math inline">\(48\)</span> bulbs and plants them to see what colour they grow. Of the <span class="math inline">\(48\)</span> planted, <span class="math inline">\(36\)</span> grow red. What should she conclude?</p>
<p>Intuitively, this result fits much better with the <span class="math inline">\(50\%\)</span> hypothesis than the <span class="math inline">\(25\%\)</span> hypothesis. So she should conclude she got the second, <span class="math inline">\(50\%\)</span> kind of shipment. It’s just a coincidence that well over half the bulbs in her experiment were red.</p>
<p>But if she uses significance testing, she won’t get this result. In fact she’ll get an <em>impossible</em> result. Let’s see how.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:unnamed-chunk-156"></span>
<img src="_main_files/figure-html/unnamed-chunk-156-1.png" alt="The result $k = 36$ out of $n = 48$ is easily statistically significant for the null hypothesis $p = .25$." width="672"  />
<!--
<p class="caption marginnote">-->Figure 20.7: The result <span class="math inline">\(k = 36\)</span> out of <span class="math inline">\(n = 48\)</span> is easily statistically significant for the null hypothesis <span class="math inline">\(p = .25\)</span>.<!--</p>-->
<!--</div>--></span>
</p>
<p>Our florist starts by testing the hypothesis that <span class="math inline">\(25\%\)</span> of the bulbs in the shipment are red. She calculates <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[
  \begin{aligned}
    \mu &amp;= np = (48)(1/4) = 12,\\
    \sigma &amp;= \sqrt{np(1-p)} = \sqrt{(48)(1/4)(3/4)} = 3.
  \end{aligned}
\]</span>
The <span class="math inline">\(99\%\)</span> range is <span class="math inline">\(12 \pm (3)(3)\)</span>, or from <span class="math inline">\(3\)</span> to <span class="math inline">\(21\)</span>. Her finding of <span class="math inline">\(k = 36\)</span> is nowhere close to this range, so the result is significant at the <span class="math inline">\(.01\)</span> level. She rejects the <span class="math inline">\(25\%\)</span> hypothesis.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:unnamed-chunk-157"></span>
<img src="_main_files/figure-html/unnamed-chunk-157-1.png" alt="The result $k = 36$ out of $n = 48$ is also statistically significant for the null hypothesis $p = .5$." width="672"  />
<!--
<p class="caption marginnote">-->Figure 20.8: The result <span class="math inline">\(k = 36\)</span> out of <span class="math inline">\(n = 48\)</span> is also statistically significant for the null hypothesis <span class="math inline">\(p = .5\)</span>.<!--</p>-->
<!--</div>--></span>
</p>
<p>So far so good, but what if she tests the <span class="math inline">\(50\%\)</span> hypothesis too? She calculates the new <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[
  \begin{aligned}
    \mu &amp;= np = (48)(1/2) = 24,\\
    \sigma &amp;= \sqrt{np(1-p)} = \sqrt{(48)(1/2)(1/2)} \approx 3.5.
  \end{aligned}
\]</span>
So her result <span class="math inline">\(k = 36\)</span> is also significant at the <span class="math inline">\(.01\)</span> level! The <span class="math inline">\(99\%\)</span> range is <span class="math inline">\(13.5\)</span> to <span class="math inline">\(34.5\)</span>, which doesn’t include <span class="math inline">\(k = 36\)</span>. So she rejects the <span class="math inline">\(50\%\)</span> hypothesis also.</p>
<p>But now she has rejected the only two possibilities. There are only two kinds of shipment, and she’s ruled them both out. Something seems to have gone wrong!</p>
<p>How did things go so wrong? Figure <a href="chlindley.html#fig:lindley">20.9</a> shows what’s happening here. Neither hypothesis fits the finding <span class="math inline">\(k = 36\)</span> very well: it’s an extremely improbable result on either hypothesis. This is why both hypotheses end up being rejected by a significance test.</p>
<div class="figure"><span id="fig:lindley"></span>
<p class="caption marginnote shownote">
Figure 20.9: An illustration of Lindley’s paradox. The finding <span class="math inline">\(k = 36\)</span> doesn’t fit well with either hypothesis, so both are rejected. But it fits one of them (red) much better than the other (blue). So the red hypothesis is more plausible from an intuitive perspective.
</p>
<img src="_main_files/figure-html/lindley-1.png" alt="An illustration of Lindley's paradox. The finding $k = 36$ doesn't fit well with either hypothesis, so both are rejected. But it fits one of them (red) much better than the other (blue). So the red hypothesis is more plausible from an intuitive perspective." width="672"  />
</div>
<p>But one of these hypotheses still fits the finding much better than the other. The blue curve (<span class="math inline">\(p = .25\)</span>) flatlines long before it gets to <span class="math inline">\(k = 36\)</span>, while the red curve (<span class="math inline">\(p = .5\)</span>) is only close to flatlining. So the most plausible interpretation is that the shipment is half red bulbs (<span class="math inline">\(p = .5\)</span>), and it’s just a fluke that we happen to have gotten much more than half red in our sample.</p>
</div>
<div id="a-bayesian-analysis" class="section level2">
<h2><span class="header-section-number">20.5</span> A Bayesian Analysis</h2>
<p><span class="newthought">Bayesians</span> will happily point out the source of the trouble: our florist has ignored the prior probabilities. If we use Bayes’ theorem instead of significance testing, we’ll find that the store owner should believe the second hypothesis, which seems right. <span class="math inline">\(36\)</span> red bulbs out of <span class="math inline">\(48\)</span> fits much better with the <span class="math inline">\(50\%\)</span> hypothesis than with the <span class="math inline">\(25\%\)</span> hypothesis.</p>
<p>How do we apply Bayes’ theorem in the tulip example? First we label our two hypotheses and the evidence:
<span class="math display">\[
  \begin{aligned}
    H      &amp;= \mbox{$25\%$ of the bulbs are red},\\
    \neg H &amp;= \mbox{$50\%$ of the bulbs are red,}\\
    E      &amp;= \mbox{Out of 48 randomly selected bulbs, 36 grew red.}
  \end{aligned}
\]</span>
Because the two kinds of shipment are equally common, the prior probabilities of our hypotheses are:
<span class="math display">\[
  \begin{aligned}
    \p(H) &amp;= 1/2,\\
    \p(\neg H) &amp;= 1/2.
  \end{aligned}
\]</span>
So we just need to calculate <span class="math inline">\(\p(E \given H)\)</span> and <span class="math inline">\(\p(E \given \neg H)\)</span>. That’s actually not so easy, but with the help of a computer we get:
<span class="math display">\[
  \begin{aligned}
    \p(E \given H) &amp;\approx 4.7 \times 10^{-13},\\
    \p(E \given \neg H) &amp;\approx 2.5 \times 10^{-4}.
  \end{aligned}
\]</span>
So we plug these numbers into Bayes’ theorem and get:
<span class="math display">\[
  \begin{aligned}
    \p(H \given E) &amp;= \frac{\p(E \given H)\p(H)}{\p(E \given H)\p(H) + \p(E \given \neg H)\p(\neg H)}\\
        &amp;\approx \frac{(4.7 \times 10^{-13})(1/2)}{(4.7 \times 10^{-13})(1/2) + (2.5 \times 10^{-4})(1/2)}\\
        &amp;\approx .000000002,\\
    \p(\neg H \given E) &amp;= \frac{\p(E \given \neg H)\p(\neg H)}{\p(E \given \neg H)\p(\neg H) + \p(E \given H)\p(H)}\\
        &amp;\approx \frac{(2.5 \times 10^{-4})(1/2)}{(2.5 \times 10^{-4})(1/2) + (4.7 \times 10^{-13})(1/2)}\\
        &amp;\approx .999999998.
  \end{aligned}
\]</span>
Conclusion: the probability of the first hypothesis <span class="math inline">\(H\)</span> has gone way down, from <span class="math inline">\(1/2\)</span> to about <span class="math inline">\(.000000002\)</span>. But the probability of the second hypothesis <span class="math inline">\(\neg H\)</span> has gone way up from <span class="math inline">\(1/2\)</span> to about <span class="math inline">\(.999999998\)</span>! So we should believe the second hypothesis, not reject it.</p>
<p>According to Bayesian critics, this shows that significance testing is misguided. It ignores crucial background information. In this example, there were only two possible hypotheses, and they were equally likely. So whichever one fits the results best is supported by the evidence. In fact, the second hypothesis is <em>strongly</em> supported by the evidence, even though it fits the result quite poorly! Sometimes it makes more sense to <a href="https://xkcd.com/1132/">reconcile ourselves to a coincidence than to reject the null hypothesis</a>.</p>
</div>
<div id="exercises-16" class="section level2 unnumbered">
<h2>Exercises</h2>
<ol>
<li><p>Which of the following statements are true? Select all that apply.</p>
<ol style="list-style-type: lower-alpha">
<li>The higher the cutoff for statistical significance, the more true hypotheses will be discovered.</li>
<li>The higher the cutoff for statistical significance, the more false hypotheses will be “discovered”.</li>
<li>The higher the cutoff for statistical significance, the more true hypotheses will go undiscovered.</li>
<li>The higher the cutoff for statistical significance, the more false hypotheses will be correctly rejected.</li>
</ol></li>
<li><p>Suppose we have <span class="math inline">\(1,000\)</span> coins and we are going to conduct an experiment on each one to determine which are biased. Each coin will be tossed <span class="math inline">\(100\)</span> times. In each experiment, the null hypothesis is always that the coin is fair, and we will reject this hypothesis when the results of the experiment are significant at the <span class="math inline">\(.05\)</span> level.</p>
<p>Suppose half the coins are fair and half are not. Suppose also that when a coin is unfair, the probability of getting a result that is not statistically significant is <span class="math inline">\(0.2\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>How many fair coins should we expect will incorrectly end up labeled “unfair”.</li>
<li>How many unfair coins should we expect will correctly end up labeled “unfair”.</li>
<li>What percentage of the coins labeled “unfair” should we expect to actually be unfair?</li>
</ol></li>
<li><p>Suppose we are going to investigate <span class="math inline">\(1,000\)</span> null hypotheses by running an experiment on each. In each experiment, we will reject the null hypothesis when the results are significant at the <span class="math inline">\(.01\)</span> level.</p>
<p>Suppose <span class="math inline">\(90\%\)</span> of our hypotheses are false. Suppose also that when a null hypothesis is false, the results will not be statistically significant <span class="math inline">\(25\%\)</span> of the time.</p>
<ol style="list-style-type: lower-alpha">
<li>How many false hypotheses should we expect will fail to be rejected?</li>
<li>What percentage of the rejected hypotheses should we expect to actually be false?</li>
</ol></li>
<li><p>True or false: it is possible for the results of an experiment to be significant at the <span class="math inline">\(.05\)</span> level even though the posterior probability of the null hypothesis is <span class="math inline">\(\p(H \given E) = .99\)</span>.</p></li>
<li><p>Suppose there are two types of urns, Type A and Type B. Type A urns contain <span class="math inline">\(1/5\)</span> black balls, the rest white. Type B urns contain <span class="math inline">\(1/10\)</span> black balls, the rest white.</p>
<p>You have an urn that could be either Type A or Type B, you aren’t sure. You think it’s equally likely to be Type A as Type B. So you decide to use a significance test to find out.</p>
<p>Your null hypothesis is that it’s a Type A urn. You draw <span class="math inline">\(25\)</span> marbles at random, and <span class="math inline">\(13\)</span> of them are black.</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation?</li>
<li>Is the result significant at the <span class="math inline">\(.01\)</span> level for this null hypothesis?</li>
<li>Suppose our null hypothesis had been instead that the urn is Type B. What would <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> be then?</li>
<li>Is the result significant at the <span class="math inline">\(.01\)</span> level for the Type B hypothesis?</li>
</ol></li>
<li><p>Suppose the government is testing a new education policy. There are only two possibilities: either the policy will work and it will help high school students learn to write better <span class="math inline">\(3/4\)</span> of the time, or it will have no effect and students’ writing will only improve <span class="math inline">\(1/2\)</span> of the time, as usual.</p>
<p>The government does a study of <span class="math inline">\(432\)</span> students and finds that, under the new policy, <span class="math inline">\(285\)</span> of them improved in their writing.</p>
<ol style="list-style-type: lower-alpha">
<li>If the null hypothesis is that each student has a <span class="math inline">\(1/2\)</span> chance of improving, are the results of the study significant at the <span class="math inline">\(.01\)</span> level?</li>
<li>If the null hypothesis is that each student has a <span class="math inline">\(3/4\)</span> chance of improving, are the results of the study significant at the <span class="math inline">\(.01\)</span> level?</li>
</ol></li>
<li><p>Lindley’s paradox occurs when a significance test will direct us to reject the hypothesis even though the posterior probability of the hypothesis <span class="math inline">\(\p(H \given E)\)</span> is high. Describe your own example of this kind of case.</p></li>
</ol>

</div>
</div>



<p style="text-align: center;">
<a href="significance-testing.html"><button class="btn btn-default">Previous</button></a>
<a href="cheat-sheet.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
