<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="10 Probability &amp; Induction | Odds &amp; Ends" />
<meta property="og:type" content="book" />

<meta property="og:image" content="/img/social_image.png" />
<meta property="og:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="github-repo" content="jweisber/vip" />

<meta name="author" content="Jonathan Weisberg" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="An open access textbook for introductory philosophy courses on probability and inductive logic.">

<title>10 Probability &amp; Induction | Odds &amp; Ends</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@jweisber" />
<meta name="twitter:creator" content="@jweisber" />
<meta name="twitter:title" content="10 Probability &amp; Induction | Odds &amp; Ends" />
<meta name="twitter:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="twitter:image" content="img/social_image.png" />


<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["output/HTML-CSS"],
  "HTML-CSS": {
    availableFonts: ["Gyre-Pagella"],
    preferredFont: "Gyre-Pagella",
    webFont: "Gyre-Pagella",
    imageFont: "Gyre-Pagella"
  }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      extensions: ["color.js"]
    }
  });
</script>





<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="custom.css" type="text/css" />

</head>

<body>


<div style="display: none;">
$$
  \newcommand{\given}{\mid}
  \renewcommand{\neg}{\mathbin{\sim}}
  \renewcommand{\wedge}{\mathbin{\&}}
  \newcommand{\p}{Pr}
  \newcommand{\degr}{^{\circ}}
  \newcommand{\E}{E}
  \newcommand{\EU}{EU}
  \newcommand{\u}{U}
  \newcommand{\pr}{Pr}
  \newcommand{\po}{Pr^*}
  \definecolor{bookred}{RGB}{228,6,19}
  \definecolor{bookblue}{RGB}{0,92,169}
  \definecolor{bookpurple}{RGB}{114,49,94} 
$$
</div>

<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li class="part"><span><b>Part I</b></span></li>
<li><a href="the-monty-hall-problem.html#the-monty-hall-problem"><span class="toc-section-number">1</span> The Monty Hall Problem</a>
<ul>
<li><a href="the-monty-hall-problem.html#diagramming-the-solution"><span class="toc-section-number">1.1</span> Diagramming the Solution</a></li>
<li><a href="the-monty-hall-problem.html#lessons"><span class="toc-section-number">1.2</span> Lessons Learned</a></li>
<li><a href="the-monty-hall-problem.html#exercises">Exercises</a></li>
</ul></li>
<li><a href="logic.html#logic"><span class="toc-section-number">2</span> Logic</a>
<ul>
<li><a href="logic.html#validity-soundness"><span class="toc-section-number">2.1</span> Validity &amp; Soundness</a></li>
<li><a href="logic.html#propositions"><span class="toc-section-number">2.2</span> Propositions</a></li>
<li><a href="logic.html#visualizing-propositions"><span class="toc-section-number">2.3</span> Visualizing Propositions</a></li>
<li><a href="logic.html#strength"><span class="toc-section-number">2.4</span> Strength</a></li>
<li><a href="logic.html#indargs"><span class="toc-section-number">2.5</span> Forms of Inductive Argument</a></li>
<li><a href="logic.html#exercises-1">Exercises</a></li>
</ul></li>
<li><a href="truth-tables.html#truth-tables"><span class="toc-section-number">3</span> Truth Tables</a>
<ul>
<li><a href="truth-tables.html#connectives"><span class="toc-section-number">3.1</span> Connectives</a></li>
<li><a href="truth-tables.html#truth-tables-1"><span class="toc-section-number">3.2</span> Truth Tables</a></li>
<li><a href="truth-tables.html#logical-truths-contradictions"><span class="toc-section-number">3.3</span> Logical Truths &amp; Contradictions</a></li>
<li><a href="truth-tables.html#mutual-exclusivity-truth-tables"><span class="toc-section-number">3.4</span> Mutual Exclusivity &amp; Truth Tables</a></li>
<li><a href="truth-tables.html#entailment-equivalence"><span class="toc-section-number">3.5</span> Entailment &amp; Equivalence</a></li>
<li><a href="truth-tables.html#summary"><span class="toc-section-number">3.6</span> Summary</a></li>
<li><a href="truth-tables.html#exercises-2">Exercises</a></li>
</ul></li>
<li><a href="the-gamblers-fallacy.html#the-gamblers-fallacy"><span class="toc-section-number">4</span> The Gambler’s Fallacy</a>
<ul>
<li><a href="the-gamblers-fallacy.html#independence"><span class="toc-section-number">4.1</span> Independence</a></li>
<li><a href="the-gamblers-fallacy.html#fairness"><span class="toc-section-number">4.2</span> Fairness</a></li>
<li><a href="the-gamblers-fallacy.html#the-gamblers-fallacy-1"><span class="toc-section-number">4.3</span> The Gambler’s Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#ignorance-is-not-a-fallacy"><span class="toc-section-number">4.4</span> Ignorance Is Not a Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#the-hot-hand-fallacy"><span class="toc-section-number">4.5</span> The Hot Hand Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#exercises-3">Exercises</a></li>
</ul></li>
<li><a href="calculating-probabilities.html#calculating-probabilities"><span class="toc-section-number">5</span> Calculating Probabilities</a>
<ul>
<li><a href="calculating-probabilities.html#multiplying-probabilities"><span class="toc-section-number">5.1</span> Multiplying Probabilities</a></li>
<li><a href="calculating-probabilities.html#adding-probabilities"><span class="toc-section-number">5.2</span> Adding Probabilities</a></li>
<li><a href="calculating-probabilities.html#exclusivity-vs.-independence"><span class="toc-section-number">5.3</span> Exclusivity vs. Independence</a></li>
<li><a href="calculating-probabilities.html#tautologies-contradictions-and-equivalent-propositions"><span class="toc-section-number">5.4</span> Tautologies, Contradictions, and Equivalent Propositions</a></li>
<li><a href="calculating-probabilities.html#the-language-of-events"><span class="toc-section-number">5.5</span> The Language of Events</a></li>
<li><a href="calculating-probabilities.html#summary-1"><span class="toc-section-number">5.6</span> Summary</a></li>
<li><a href="calculating-probabilities.html#exercises-4">Exercises</a></li>
</ul></li>
<li><a href="conditional-probability.html#conditional-probability"><span class="toc-section-number">6</span> Conditional Probability</a>
<ul>
<li><a href="conditional-probability.html#calculating-conditional-probability"><span class="toc-section-number">6.1</span> Calculating Conditional Probability</a></li>
<li><a href="conditional-probability.html#conditional-probability-trees"><span class="toc-section-number">6.2</span> Conditional Probability &amp; Trees</a></li>
<li><a href="conditional-probability.html#more-examples"><span class="toc-section-number">6.3</span> More Examples</a></li>
<li><a href="conditional-probability.html#order-matters"><span class="toc-section-number">6.4</span> Order Matters</a></li>
<li><a href="conditional-probability.html#declaring-independence"><span class="toc-section-number">6.5</span> Declaring Independence</a></li>
<li><a href="conditional-probability.html#ch6ex">Exercises</a></li>
</ul></li>
<li><a href="calculating-probabilities-part-2.html#calculating-probabilities-part-2"><span class="toc-section-number">7</span> Calculating Probabilities, Part 2</a>
<ul>
<li><a href="calculating-probabilities-part-2.html#the-negation-rule"><span class="toc-section-number">7.1</span> The Negation Rule</a></li>
<li><a href="calculating-probabilities-part-2.html#the-general-addition-rule"><span class="toc-section-number">7.2</span> The General Addition Rule</a></li>
<li><a href="calculating-probabilities-part-2.html#the-general-multiplication-rule"><span class="toc-section-number">7.3</span> The General Multiplication Rule</a></li>
<li><a href="calculating-probabilities-part-2.html#laplaces-urn-puzzle"><span class="toc-section-number">7.4</span> Laplace’s Urn Puzzle</a></li>
<li><a href="calculating-probabilities-part-2.html#the-law-of-total-probability"><span class="toc-section-number">7.5</span> The Law of Total Probability</a></li>
<li><a href="calculating-probabilities-part-2.html#example"><span class="toc-section-number">7.6</span> Example</a></li>
<li><a href="calculating-probabilities-part-2.html#exercises-5">Exercises</a></li>
</ul></li>
<li><a href="chbayes.html#chbayes"><span class="toc-section-number">8</span> Bayes’ Theorem</a>
<ul>
<li><a href="chbayes.html#bayes-theorem"><span class="toc-section-number">8.1</span> Bayes’ Theorem</a></li>
<li><a href="chbayes.html#understanding-bayes-theorem"><span class="toc-section-number">8.2</span> Understanding Bayes’ Theorem</a></li>
<li><a href="chbayes.html#bayes-long-theorem"><span class="toc-section-number">8.3</span> Bayes’ Long Theorem</a></li>
<li><a href="chbayes.html#example-1"><span class="toc-section-number">8.4</span> Example</a></li>
<li><a href="chbayes.html#baserate"><span class="toc-section-number">8.5</span> The Base Rate Fallacy</a></li>
<li><a href="chbayes.html#exercises-6">Exercises</a></li>
</ul></li>
<li><a href="multiple-conditions.html#multiple-conditions"><span class="toc-section-number">9</span> Multiple Conditions</a>
<ul>
<li><a href="multiple-conditions.html#multiple-draws"><span class="toc-section-number">9.1</span> Multiple Draws</a></li>
<li><a href="multiple-conditions.html#multiple-witnesses"><span class="toc-section-number">9.2</span> Multiple Witnesses</a></li>
<li><a href="multiple-conditions.html#without-replacement"><span class="toc-section-number">9.3</span> Without Replacement</a></li>
<li><a href="multiple-conditions.html#multiplying-conditional-probabilities"><span class="toc-section-number">9.4</span> Multiplying Conditional Probabilities</a></li>
<li><a href="multiple-conditions.html#summary-2"><span class="toc-section-number">9.5</span> Summary</a></li>
<li><a href="multiple-conditions.html#exercises-7">Exercises</a></li>
</ul></li>
<li><a href="probability-induction.html#probability-induction"><span class="toc-section-number">10</span> Probability &amp; Induction</a>
<ul>
<li><a href="probability-induction.html#generalizing-from-observed-instances"><span class="toc-section-number">10.1</span> Generalizing from Observed Instances</a></li>
<li><a href="probability-induction.html#real-life-is-more-complicated"><span class="toc-section-number">10.2</span> Real Life Is More Complicated</a></li>
<li><a href="probability-induction.html#succession"><span class="toc-section-number">10.3</span> The Rule of Succession</a></li>
<li><a href="probability-induction.html#bayesibe"><span class="toc-section-number">10.4</span> Inference to the Best Explanation</a></li>
<li><a href="probability-induction.html#exercises-8">Exercises</a></li>
</ul></li>
<li class="part"><span><b>Part II</b></span></li>
<li><a href="expected-value.html#expected-value"><span class="toc-section-number">11</span> Expected Value</a>
<ul>
<li><a href="expected-value.html#expected-monetary-values"><span class="toc-section-number">11.1</span> Expected Monetary Values</a></li>
<li><a href="expected-value.html#visualizing-expectations"><span class="toc-section-number">11.2</span> Visualizing Expectations</a></li>
<li><a href="expected-value.html#more-than-two-outcomes"><span class="toc-section-number">11.3</span> More Than Two Outcomes</a></li>
<li><a href="expected-value.html#fair-prices"><span class="toc-section-number">11.4</span> Fair Prices</a></li>
<li><a href="expected-value.html#other-goods"><span class="toc-section-number">11.5</span> Other Goods</a></li>
<li><a href="expected-value.html#decision-tables"><span class="toc-section-number">11.6</span> Decision Tables</a></li>
<li><a href="expected-value.html#dominance"><span class="toc-section-number">11.7</span> Dominance</a></li>
<li><a href="expected-value.html#exercises-9">Exercises</a></li>
</ul></li>
<li><a href="utility.html#utility"><span class="toc-section-number">12</span> Utility</a>
<ul>
<li><a href="utility.html#subjectivity-objectivity"><span class="toc-section-number">12.1</span> Subjectivity &amp; Objectivity</a></li>
<li><a href="utility.html#the-general-recipe"><span class="toc-section-number">12.2</span> The General Recipe</a></li>
<li><a href="utility.html#choosing-scales"><span class="toc-section-number">12.3</span> Choosing Scales</a></li>
<li><a href="utility.html#a-limitation-the-expected-utility-assumption"><span class="toc-section-number">12.4</span> A Limitation: The Expected Utility Assumption</a></li>
<li><a href="utility.html#the-value-of-money"><span class="toc-section-number">12.5</span> The Value of Money</a></li>
<li><a href="utility.html#exercises-10">Exercises</a></li>
</ul></li>
<li><a href="challenges-to-expected-utility.html#challenges-to-expected-utility"><span class="toc-section-number">13</span> Challenges to Expected Utility</a>
<ul>
<li><a href="challenges-to-expected-utility.html#the-allais-paradox"><span class="toc-section-number">13.1</span> The Allais Paradox</a></li>
<li><a href="challenges-to-expected-utility.html#the-sure-thing-principle"><span class="toc-section-number">13.2</span> The Sure-thing Principle</a></li>
<li><a href="challenges-to-expected-utility.html#prescriptive-vs.-descriptive"><span class="toc-section-number">13.3</span> Prescriptive vs. Descriptive</a></li>
<li><a href="challenges-to-expected-utility.html#the-ellsberg-paradox"><span class="toc-section-number">13.4</span> The Ellsberg Paradox</a></li>
<li><a href="challenges-to-expected-utility.html#ellsberg-allais"><span class="toc-section-number">13.5</span> Ellsberg &amp; Allais</a></li>
<li><a href="challenges-to-expected-utility.html#exercises-11">Exercises</a></li>
</ul></li>
<li><a href="infinity-beyond.html#infinity-beyond"><span class="toc-section-number">14</span> Infinity &amp; Beyond</a>
<ul>
<li><a href="infinity-beyond.html#the-st.-petersburg-paradox"><span class="toc-section-number">14.1</span> The St. Petersburg Paradox</a></li>
<li><a href="infinity-beyond.html#bernoullis-solution"><span class="toc-section-number">14.2</span> Bernoulli’s Solution</a></li>
<li><a href="infinity-beyond.html#st.-petersburgs-revenge"><span class="toc-section-number">14.3</span> St. Petersburg’s Revenge</a></li>
<li><a href="infinity-beyond.html#pascals-wager"><span class="toc-section-number">14.4</span> Pascal’s Wager</a></li>
<li><a href="infinity-beyond.html#responses-to-pascals-wager"><span class="toc-section-number">14.5</span> Responses to Pascal’s Wager</a></li>
<li><a href="infinity-beyond.html#exercises-12">Exercises</a></li>
</ul></li>
<li class="part"><span><b>Part III</b></span></li>
<li><a href="two-schools.html#two-schools"><span class="toc-section-number">15</span> Two Schools</a>
<ul>
<li><a href="two-schools.html#probability-as-frequency"><span class="toc-section-number">15.1</span> Probability as Frequency</a></li>
<li><a href="two-schools.html#probability-as-belief"><span class="toc-section-number">15.2</span> Probability as Belief</a></li>
<li><a href="two-schools.html#which-kind-of-probability"><span class="toc-section-number">15.3</span> Which Kind of Probability?</a></li>
<li><a href="two-schools.html#frequentism"><span class="toc-section-number">15.4</span> Frequentism</a></li>
<li><a href="two-schools.html#bayesianism"><span class="toc-section-number">15.5</span> Bayesianism</a></li>
<li><a href="two-schools.html#exercises-13">Exercises</a></li>
</ul></li>
<li><a href="beliefs-betting-rates.html#beliefs-betting-rates"><span class="toc-section-number">16</span> Beliefs &amp; Betting Rates</a>
<ul>
<li><a href="beliefs-betting-rates.html#measuring-personal-probabilities"><span class="toc-section-number">16.1</span> Measuring Personal Probabilities</a></li>
<li><a href="beliefs-betting-rates.html#things-to-watch-out-for"><span class="toc-section-number">16.2</span> Things to Watch Out For</a></li>
<li><a href="beliefs-betting-rates.html#indirect-measurements"><span class="toc-section-number">16.3</span> Indirect Measurements</a></li>
<li><a href="beliefs-betting-rates.html#exercises-14">Exercises</a></li>
</ul></li>
<li><a href="dutch-books.html#dutch-books"><span class="toc-section-number">17</span> Dutch Books</a>
<ul>
<li><a href="dutch-books.html#dutch-books-1"><span class="toc-section-number">17.1</span> Dutch Books</a></li>
<li><a href="dutch-books.html#bankteller"><span class="toc-section-number">17.2</span> The Bankteller Fallacy</a></li>
<li><a href="dutch-books.html#dutch-books-in-general"><span class="toc-section-number">17.3</span> Dutch Books in General</a></li>
<li><a href="dutch-books.html#exercises-15">Exercises</a></li>
</ul></li>
<li><a href="priors.html#priors"><span class="toc-section-number">18</span> The Problem of Priors</a>
<ul>
<li><a href="priors.html#priors-posteriors"><span class="toc-section-number">18.1</span> Priors &amp; Posteriors</a></li>
<li><a href="priors.html#the-principle-of-indifference"><span class="toc-section-number">18.2</span> The Principle of Indifference</a></li>
<li><a href="priors.html#the-continuous-principle-of-indifference"><span class="toc-section-number">18.3</span> The Continuous Principle of Indifference</a></li>
<li><a href="priors.html#bertrands-paradox"><span class="toc-section-number">18.4</span> Bertrand’s Paradox</a></li>
<li><a href="priors.html#the-problem-of-priors"><span class="toc-section-number">18.5</span> The Problem of Priors</a></li>
<li><a href="priors.html#exercises-16">Exercises</a></li>
</ul></li>
<li><a href="significance-testing.html#significance-testing"><span class="toc-section-number">19</span> Significance Testing</a>
<ul>
<li><a href="significance-testing.html#statistical-significance"><span class="toc-section-number">19.1</span> Statistical Significance</a></li>
<li><a href="significance-testing.html#levels-of-significance"><span class="toc-section-number">19.2</span> Levels of Significance</a></li>
<li><a href="significance-testing.html#normal-approximation"><span class="toc-section-number">19.3</span> Normal Approximation</a></li>
<li><a href="significance-testing.html#the-68-95-99-rule"><span class="toc-section-number">19.4</span> The 68-95-99 Rule</a></li>
<li><a href="significance-testing.html#binomial-probabilities"><span class="toc-section-number">19.5</span> Binomial Probabilities</a></li>
<li><a href="significance-testing.html#foobar"><span class="toc-section-number">19.6</span> Significance Testing</a></li>
<li><a href="significance-testing.html#warnings"><span class="toc-section-number">19.7</span> Warnings</a></li>
<li><a href="significance-testing.html#exercises-17">Exercises</a></li>
</ul></li>
<li><a href="chlindley.html#chlindley"><span class="toc-section-number">20</span> Lindley’s Paradox</a>
<ul>
<li><a href="chlindley.html#significance-subjectivity"><span class="toc-section-number">20.1</span> Significance &amp; Subjectivity</a></li>
<li><a href="chlindley.html#the-role-of-priors-in-significance-testing"><span class="toc-section-number">20.2</span> The Role of Priors in Significance Testing</a></li>
<li><a href="chlindley.html#lindleys-paradox"><span class="toc-section-number">20.3</span> Lindley’s Paradox</a></li>
<li><a href="chlindley.html#a-bayesian-analysis"><span class="toc-section-number">20.4</span> A Bayesian Analysis</a></li>
<li><a href="chlindley.html#exercises-18">Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li><a href="cheat-sheet.html#cheat-sheet"><span class="toc-section-number">A</span> Cheat Sheet</a>
<ul>
<li><a href="cheat-sheet.html#deductive-logic">Deductive Logic</a></li>
<li><a href="cheat-sheet.html#probability">Probability</a></li>
<li><a href="cheat-sheet.html#decision-theory">Decision Theory</a></li>
<li><a href="cheat-sheet.html#bayesianism-1">Bayesianism</a></li>
<li><a href="cheat-sheet.html#frequentism-1">Frequentism</a></li>
</ul></li>
<li><a href="the-axioms-of-probability.html#the-axioms-of-probability"><span class="toc-section-number">B</span> The Axioms of Probability</a>
<ul>
<li><a href="the-axioms-of-probability.html#theories-and-axioms">Theories and Axioms</a></li>
<li><a href="the-axioms-of-probability.html#the-three-axioms-of-probability">The Three Axioms of Probability</a></li>
<li><a href="the-axioms-of-probability.html#first-steps">First Steps</a></li>
<li><a href="the-axioms-of-probability.html#conditional-probability-the-multiplication-rule">Conditional Probability &amp; the Multiplication Rule</a></li>
<li><a href="the-axioms-of-probability.html#equivalence-general-addition">Equivalence &amp; General Addition</a></li>
<li><a href="the-axioms-of-probability.html#total-probability-bayes-theorem">Total Probability &amp; Bayes’ Theorem</a></li>
<li><a href="the-axioms-of-probability.html#independence-1">Independence</a></li>
</ul></li>
<li><a href="grue.html#grue"><span class="toc-section-number">C</span> The Grue Paradox</a>
<ul>
<li><a href="grue.html#a-gruesome-concept">A Gruesome Concept</a></li>
<li><a href="grue.html#the-paradox">The Paradox</a></li>
<li><a href="grue.html#grue-artificial-intelligence">Grue &amp; Artificial Intelligence</a></li>
<li><a href="grue.html#disjunctivitis">Disjunctivitis</a></li>
<li><a href="grue.html#time-dependence">Time Dependence</a></li>
<li><a href="grue.html#the-moral">The Moral</a></li>
<li><a href="grue.html#exercises-19">Exercises</a></li>
</ul></li>
<li><a href="the-problem-of-induction.html#the-problem-of-induction"><span class="toc-section-number">D</span> The Problem of Induction</a>
<ul>
<li><a href="the-problem-of-induction.html#the-dilemma">The Dilemma</a></li>
<li><a href="the-problem-of-induction.html#the-problem-of-induction-vs.-the-grue-paradox">The Problem of Induction vs. the Grue Paradox</a></li>
<li><a href="the-problem-of-induction.html#probability-theory-to-the-rescue">Probability Theory to the Rescue?</a></li>
<li><a href="the-problem-of-induction.html#exercises-20">Exercises</a></li>
</ul></li>
<li><a href="solutions-to-selected-exercises.html#solutions-to-selected-exercises"><span class="toc-section-number">E</span> Solutions to Selected Exercises</a>
<ul>
<li><a href="solutions-to-selected-exercises.html#chapter-1">Chapter 1</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-2">Chapter 2</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-3">Chapter 3</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-4">Chapter 4</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-5">Chapter 5</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-6">Chapter 6</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-7">Chapter 7</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-8">Chapter 8</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-9">Chapter 9</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-11">Chapter 11</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-12">Chapter 12</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-13">Chapter 13</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-14">Chapter 14</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-16">Chapter 16</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-17">Chapter 17</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-18">Chapter 18</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-19">Chapter 19</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-20">Chapter 20</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="probability-induction" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Probability &amp; Induction</h1>
<div class="epigraph">
<p>
Nothing in life is to be feared, it is only to be understood. Now is the time to understand more, so that we may fear less.<br />
—Marie Curie
</p>
</div>
<p><span class="newthought">We</span> met some common types of inductive argument back in <a href="logic.html#indargs">Chapter 2</a>. Now that we know how to work with probability, let’s use what we’ve learned to sharpen our understanding of how those arguments work.</p>
<div id="generalizing-from-observed-instances" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Generalizing from Observed Instances</h2>
<p>Generalizing from observed instances was the first major form of inductive argument we encountered. Suppose you want to know what colour a particular species of bird tends to be. Then you might go out and look at a bunch of examples:</p>
<div class="argument">
<p>
I’ve seen <span class="math inline"><span class="math inline">\(10\)</span></span> ravens and they’ve all been black.<br />
Therefore, all ravens are black.
</p>
</div>
<p>How strong is this argument?</p>
<p>Observing ravens is a lot like sampling from an urn. Each raven is a marble, and the population of all ravens is the urn. We don’t know what nature’s urn contains at first: it might contain only black ravens, or it might contain ravens of other colours too. To assess the argument’s strength, we have to calculate <span class="math inline">\(\p(A \given B_1 \wedge B_2 \wedge \ldots \wedge B_{10})\)</span>: the probability that all ravens in nature’s urn are black, given that the first raven we observed was black, and the second, and so on, up to the tenth raven.</p>
<p>We learned how to solve simple problems of this form in the <a href="multiple-conditions.html#multiple-draws">previous chapter</a>. For example, imagine you face another of our mystery urns, and this time there are two equally likely possibilities.
<span class="math display">\[
  \begin{aligned}
    A      &amp;= \mbox{The urn contains only black marbles.} \\
    \neg A &amp;= \mbox{The urn contains an equal mix of black and white marbles.} \\
  \end{aligned}
\]</span>
If we do two random draws with replacement, and both are black, we calculate <span class="math inline">\(\p(A \given B_1 \wedge B_2)\)</span> using Bayes’ theorem:
<span class="math display">\[
  \begin{aligned}
    \p(A \given B_1 \wedge B_2) &amp;= \frac{\p(B_1 \wedge B_2 \given A)\p(A)}{\p(B_1 \wedge B_2 \given A) \p(A) + \p(B_1 \wedge B_2 \given \neg A) \p(\neg A)} \\ 
    &amp;= \frac{(1)^2(1/2)}{(1)^2(1/2) + (1/2)^2(1/2)}\\
    &amp;= 4/5.
  \end{aligned}
\]</span>
If we do a third draw with replacement, and it too comes up black, we replace the squares with cubes. On the fourth draw we’d raise to the fourth power. And so on. When we get to the tenth black draw, the calculation becomes:
<span class="math display">\[
  \begin{aligned}
    \p(A \given B_1 \wedge \ldots \wedge B_{10}) &amp;= \frac{(1)^{10}(1/2)}{(1)^{10}(1/2) + (1/2)^{10}(1/2)}\\
    &amp;= 1,024/1,025\\
    &amp;\approx .999.
  \end{aligned}
\]</span>
So after ten black draws, we can be about <span class="math inline">\(99.9\%\)</span> certain the urn contains only black marbles.</p>
<p>But that doesn’t mean our argument that all ravens are black is <span class="math inline">\(99.9\%\)</span> strong!</p>
</div>
<div id="real-life-is-more-complicated" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Real Life Is More Complicated</h2>
<p>There are two major limitations to our urn analogy.</p>
<p><span class="newthought">The first</span> limitation is that the ravens we observe in real life aren’t randomly sampled from nature’s “urn.” We only observe ravens in certain locations, for example. But our solution to the urn problem relied on random sampling. For example, we assumed <span class="math inline">\(\p(B_1 \given \neg A) = 1/2\)</span> because the black marbles are just as likely to be drawn as the white ones, if there are any white ones.</p>
<p>If there are white ravens in the world though, they might be limited to certain locales.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> In fact there are white ravens, <a href="https://vancouversun.com/news/local-news/rare-white-raven-spotted-on-vancouver-island">especially in one area of Vancouver Island</a>.</span> So the fact we’re only observing ravens in our part of the world could make a big difference to what we find. It matters whether your sample really is random.</p>
<p><span class="newthought">The second</span> limitation is that we pretended there were only two possibilities: either all the marbles in the urn are black, or half of them are. And, accordingly, we assumed there was already a <span class="math inline">\(1/2\)</span> chance all the marbles are black, before we even looked at any of them.</p>
<p>In real life though, when we encounter a new species, it could be that <span class="math inline">\(90\%\)</span> of them are black, or <span class="math inline">\(31\%\)</span>, or <span class="math inline">\(42.718\%\)</span>, or any portion from <span class="math inline">\(0\%\)</span> to <span class="math inline">\(100\%\)</span>. So there are many, many more possibilities. The possibility that <em>all</em> members of the new species (<span class="math inline">\(100\%\)</span>) are black is just one of these many possibilities. So it would start with a much lower probability than <span class="math inline">\(1/2\)</span>.</p>
</div>
<div id="succession" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> The Rule of Succession</h2>
<p>There is a famous formula that addresses this second issue.
<label for="tufte-mn-18" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-18" class="margin-toggle"><span class="marginnote"><span style="display: block;">The formula was first derived by <a href="logic.html#fig:laplace">Laplace</a> to solve <a href="https://en.wikipedia.org/wiki/Sunrise_problem">the sunrise problem</a>, the problem of calculating the probability that the sun will rise tomorrow given that it’s risen every day so far.</span></span>
Suppose we take all possible compositions of the urn into account: the portion of black balls could be anywhere from <span class="math inline">\(0\%\)</span> to <span class="math inline">\(100\%\)</span>. If all these possibilities are equally likely, and we draw randomly with replacement, then the probability the next draw will be black is
<span class="math display">\[ \frac{k + 1}{n + 2},\]</span>
where <span class="math inline">\(k\)</span> is the number of black marbles drawn so far, and <span class="math inline">\(n\)</span> is the total number of draws so far. Deriving this formula is a bit tedious so we won’t go into it here. We’ll settle for understanding it instead.</p>
<p>In our example, we did <span class="math inline">\(10\)</span> draws, all of which came up black. So <span class="math inline">\(n = 10\)</span>, and <span class="math inline">\(k = 10\)</span> too. Applying the Rule of Sucession gives us a probability of
<span class="math display">\[ \frac{10 + 1}{10 + 2} = \frac{11}{12}, \]</span>
in other words the next draw has about a <span class="math inline">\(0.92\)</span> probability of being black. If we’d only gotten <span class="math inline">\(k = 5\)</span> black marbles out of <span class="math inline">\(n = 10\)</span> draws, the probability would be <span class="math inline">\(6/12 = 1/2\)</span>.</p>
<p>Notice though that we’ve somewhat changed the subject. The Rule of Succession gives us the probability for <em>one</em> draw. It doesn’t tell us the probability that <em>all</em> marbles in the urn are black, it just tells us the probability of getting a black marble if we draw one of them. Analyzing the probability that all the marbles are black is trickier, so we won’t go into it. Just be aware that the Rule of Succession gives us individual probabilities, not general ones.</p>
<p>Notice also that the Rule of Succession relies on two assumptions. The first is an assumption we also used earlier, namely that we’re sampling randomly. Sometimes this assumption is realistic, but in many real-world applications getting a random sample is tricky, even impossible.</p>
<p>The second assumption is that all possible compositions of the urn are equally likely. It’s just as likely that <span class="math inline">\(50\%\)</span> of the marbles are black as that <span class="math inline">\(75\%\)</span> are, or <span class="math inline">\(35.12\%\)</span>, or <span class="math inline">\(0.0001\%\)</span>. This assumption certainly looks reasonable, at least in some cases. But we’ll see in <a href="priors.html#priors">Chapter 18</a> that there are fundamental problems lurking here.</p>
<p>When these assumptions hold though, the Rule of Succession is entirely correct. It just follows from the rules of probability we’ve already learned. We just need to remember that it would be a mistake to use the Rule of Succession in situations where these assumptions do not apply.</p>
</div>
<div id="bayesibe" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Inference to the Best Explanation</h2>
<p>Let’s set aside arguments that generalize from observed instances, and focus instead on a different form of inductive argument we met in <a href="logic.html#logic">Chapter 2</a>, namely Inference to the Best Explanation. An example:</p>
<div class="argument">
<p>
My car won’t start and the gas gauge reads empty.<br />
Therefore, my car is out of gas.
</p>
</div>
<p>My car being out of gas is a very good explanation of the facts that it won’t start and the gauge reads empty. So this seems like a pretty strong argument.</p>
<p>How do we understand its strength using probability? This is actually a controversial topic, currently being studied by researchers. There are different, competing theories about how Inference to the Best Explanation fits into probability theory. So we’ll just look at one, popular way of understanding things.</p>
<p>Let’s start by thinking about what makes an explanation a good one.</p>
<p><span class="newthought">A good</span> explanation should account for all the things we’re trying to explain. For example, if we’re trying to explain why my car won’t start and the gauge reads empty, I’d be skeptical if my mechanic said it’s because the brakes are broken. That doesn’t account for any of the symptoms! I’d also be skeptical if they said the gas gauge was broken. That might fit okay with one of the symptoms (the gauge reads empty), but it doesn’t account for the fact the car won’t start.</p>
<p>The explanation that my car is out of gas, however, fits both symptoms. It would account for both the empty reading on the gauge and the car’s refusal to start.</p>
<p>A good explanation should also fit with other things I know. For example, suppose my mechanic tries to explain my car troubles by saying that both the gauge and the ignition broke down at the same time. But I know my car is new, it’s a highly reliable model, and it was recently serviced. So my mechanic’s explanation doesn’t fit well with the other things I know. It’s not a very good explanation.</p>
<p>We have two criteria now for a good explanation:</p>
<ol style="list-style-type: decimal">
<li>it should account for all the things we’re trying to explain, and</li>
<li>it should fit well with other things we know.</li>
</ol>
<p>These criteria match up with terms in Bayes’ theorem. Imagine we have some evidence <span class="math inline">\(E\)</span> we’re trying to explain, and some hypothesis <span class="math inline">\(H\)</span> that’s meant to explain it. Bayes’ theorem says:
<span class="math display">\[ \p(H \given E) = \frac{\p(H)\p(E \given H)}{\p(E)}. \]</span>
How probable is our explanation <span class="math inline">\(H\)</span> given our evidence <span class="math inline">\(E\)</span>? Well, the larger the terms in the numerator are, the higher that probability is. And the terms in the numerator correspond to our two criteria for a good explanation.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\p(E \given H)\)</span> corresponds to how well our hypothesis <span class="math inline">\(H\)</span> accounts for our evidence <span class="math inline">\(E\)</span>. If <span class="math inline">\(H\)</span> is the hypothesis that the car is out of gas, then <span class="math inline">\(\p(E \given H) \approx 1\)</span>. After all, if there’s no gas in the car, it’s virtually guaranteed that it won’t start and the gauge will read empty. (It’s not perfectly guaranteed because the gauge could be broken after all, though that’s not very likely.)</p></li>
<li><p><span class="math inline">\(\p(H)\)</span> corresponds to how well our hypothesis fits with other things we know. For example, suppose I know it’s been a while since I put gas in the car. If <span class="math inline">\(H\)</span> is the hypothesis that the car is out of gas, this fits well with what I already know, so <span class="math inline">\(\p(H)\)</span> will be pretty high.</p>
<p>Whereas if <span class="math inline">\(H\)</span> is the hypothesis that the gauge and the ignition both broke down at the same time, this hypothesis starts out pretty improbable given what else I know (it’s a new car, a reliable model, and recently serviced). So in that case, <span class="math inline">\(\p(H)\)</span> would be low</p></li>
</ol>
<p>So the better <span class="math inline">\(H\)</span> accounts for the evidence, the larger <span class="math inline">\(\p(E \given H)\)</span> will be. And the better <span class="math inline">\(H\)</span> fits with my background information, the larger <span class="math inline">\(\p(H)\)</span> will be. Thus, the better <span class="math inline">\(H\)</span> is as an explanation, the larger <span class="math inline">\(\p(H \given E)\)</span> will be. And thus the stronger <span class="math inline">\(E\)</span> will be as an argument for <span class="math inline">\(H\)</span>.</p>
<p>What about the last term in Bayes’ theorem though, the denominator <span class="math inline">\(\p(E)\)</span>? It corresponds to a virtue of good explanations too!</p>


<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:unnamed-chunk-86"></span>
<img src="img/moon.gif" alt="The hammer/feather experiment was performed on the moon in 1971. See the full video here."  />
<!--
<p class="caption marginnote">-->Figure 10.1: The hammer/feather experiment was performed on the moon in 1971. See the <a href="https://bit.ly/1KLQzOB">full video here</a>.<!--</p>-->
<!--</div>--></span>
</p>
<p>Scientists love theories that explain the unexplained. For example, Newton’s theory of physics is able to explain why a heavy object and a light object, like a hammer and feather, fall to the ground at the same speed as long as there’s no air resistance. If you’d never performed this experiment before, you’d probably expect the hammer to fall faster. You’d be surprised to find that the hammer and feather actually hit the ground at the same time. That Newton’s theory explains this surprising fact strongly supports his theory.</p>
<p>So the ability to explain surprising facts is a third virtue of a good explanation. And this virtue corresponds to our third term in Bayes’ theorem:</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:unnamed-chunk-87"></span>
<img src="img/vacuum.gif" alt="The hammer/feather experiment has also been performed in vacuum chambers here on earth. A beautifully filmed example is available on YouTube, courtesy of the BBC."  />
<!--
<p class="caption marginnote">-->Figure 10.2: The hammer/feather experiment has also been performed in vacuum chambers here on earth. A beautifully filmed example is <a href="https://bit.ly/10hw8mP">available on YouTube</a>, courtesy of the BBC.<!--</p>-->
<!--</div>--></span>
</p>
<ol start="3" style="list-style-type: decimal">
<li><span class="math inline">\(\p(E)\)</span> corresponds to how surprising the evidence <span class="math inline">\(E\)</span> is. If <span class="math inline">\(E\)</span> is surprising, then <span class="math inline">\(\p(E)\)</span> will be low, since <span class="math inline">\(E\)</span> isn’t something we expect to be true.</li>
</ol>
<p>And since <span class="math inline">\(\p(E)\)</span> is in the denominator of Bayes’ theorem, a smaller number there means a <em>bigger</em> value for <span class="math inline">\(\p(H \given E)\)</span>. So the more surprising the finding <span class="math inline">\(E\)</span> is, the more it supports a hypothesis <span class="math inline">\(H\)</span> that explains it.</p>
<p>According to this analysis then, each term in Bayes’ theorem corresponds to a virtue of a good explanation. And that’s why Inference to the Best Explanation works as a form of inductive inference.</p>
</div>
<div id="exercises-8" class="section level2 unnumbered">
<h2>Exercises</h2>
<ol>
<li><p>The Rule of Succession can look a little strange or mysterious at first. Why is there a <span class="math inline">\(+1\)</span> in the numerator, and a <span class="math inline">\(+2\)</span> in the denominator? To answer these questions, respond to the following.</p>
<ol style="list-style-type: lower-alpha">
<li>According to the Rule of Succession, what is the probability that the first marble drawn will be black? (Hint: in this scenario <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span> are the same number, what number?)</li>
<li>Suppose we do <span class="math inline">\(n\)</span> random draws and half of them are black. According to the Rule of Sucession, what is the probability that the next marble will be black?</li>
<li>Explain why the Rule of Succession has <span class="math inline">\(+1\)</span> in the numerator and <span class="math inline">\(+2\)</span> in the denominator.</li>
</ol></li>
<li><p>The Rule of Succession doesn’t just tell us the probability of black on the next draw. The same formula <span class="math inline">\((k+1)/(n+2)\)</span> applies to any future draw.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose we’ve done <span class="math inline">\(10\)</span> draws so far and <span class="math inline">\(7\)</span> of them were black. What is the probability that the next two draws will both be black? (Careful: the draws are not independent.)</li>
<li>In general, if we’ve done <span class="math inline">\(n\)</span> draws and <span class="math inline">\(k\)</span> were black, what is the probability that the next two draws will both be black? Your answer should be a formula in terms of <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span>.</li>
</ol></li>
<li><p>Instead of the Rule of Succession, statisticians often use the simpler formula <span class="math inline">\(k/n\)</span>. This is known as the “maximum likelihood estimate,” or MLE.</p>
<ol style="list-style-type: lower-alpha">
<li>According to the MLE, what is the probability that the first marble drawn will be black?</li>
<li>According to the MLE, if we draw one random marble and it’s white, what is the probability the next marble drawn will be black?</li>
<li>The MLE usually gives different answers than the Rule of Succession. But they do agree in one special case: when half the draws are black, both formulas equal <span class="math inline">\(1/2\)</span>. Prove that this is always true.</li>
<li>Although the two formulas usually give different answers, they give very similar answers as long as… (fill in the blank)</li>
</ol></li>
<li><p>Suppose an urn contains a mix of black and white balls. There are two, equally likely possibilities: the ratio of black to white is either <span class="math inline">\(2:1\)</span> or <span class="math inline">\(1:2\)</span>. Suppose we do two draws and they are both black.</p>
<ol style="list-style-type: lower-alpha">
<li>What is the probability the next draw will be black?</li>
<li>Would the Rule of Succession give the same answer? Why or why not?</li>
</ol></li>
<li><p>Suppose we have some evidence <span class="math inline">\(E\)</span> for which we are considering two possible explanations, <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span> are mutually exclusive and exhaustive, and they fit equally well with our background information. But <span class="math inline">\(H_1\)</span> fits the evidence <span class="math inline">\(E\)</span> better than <span class="math inline">\(H_2\)</span> does. Prove that <span class="math inline">\(\p(H_1 \given E) &gt; \p(H_2 \given E)\)</span>.</li>
<li>Suppose <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span> fit equally well with our background information, but they are not mutually exclusive or exhaustive. As before, <span class="math inline">\(H_1\)</span> fits the evidence <span class="math inline">\(E\)</span> better than <span class="math inline">\(H_2\)</span>. Prove that <span class="math inline">\(\p(H_1 \given E) &gt; \p(H_2 \given E)\)</span>.</li>
<li>Suppose the two explanations fit the evidence <span class="math inline">\(E\)</span> equally well, but <span class="math inline">\(H_1\)</span> fits better with our background information. Prove that <span class="math inline">\(\p(H_1 \given E) &gt; \p(H_2 \given E)\)</span>.</li>
</ol></li>
<li><p>Suppose we flip a coin <span class="math inline">\(3\)</span> times. Our evidence <span class="math inline">\(E\)</span> is that it lands heads, tails, heads. Now consider two possible explanations.</p>
<ul>
<li><span class="math inline">\(H_1\)</span>: the coin is fair.</li>
<li><span class="math inline">\(H_2\)</span>: the coin is biased toward heads, with a <span class="math inline">\(2/3\)</span> chance of heads on each toss (the tosses are independent).</li>
</ul>
<p>Suppose these are the only two possibilities, and they fit equally well with our background information.</p>
<ol style="list-style-type: lower-alpha">
<li>How well does each hypothesis fit the evidence, <span class="math inline">\(E\)</span>? That is, what are <span class="math inline">\(\p(E \given H_1)\)</span> and <span class="math inline">\(\p(E \given H_2)\)</span>?</li>
<li>How probable is each hypothesis given the evidence. In other words, what are <span class="math inline">\(\p(H_1 \given E)\)</span> and <span class="math inline">\(\p(H_2 \given E)\)</span>?</li>
</ol></li>
<li><p>Suppose <span class="math inline">\(\p(H \given E) &lt; \p(H)\)</span>. When is <span class="math inline">\(\p(E \given H)/\p(E) &gt; 1\)</span> then? Always, just sometimes, or never? Assume all conditional probabilities are well-defined.</p></li>
<li><p>Suppose <span class="math inline">\(\p(E \given H) &gt; \p(E)\)</span>. When is <span class="math inline">\(\p(H \given E) &gt; 1/2\)</span> then? Assume all conditional probabilities are well-defined.</p>
<ol style="list-style-type: lower-alpha">
<li>Always</li>
<li>Just sometimes: when <span class="math inline">\(\p(E) \leq 1/2\)</span>.</li>
<li>Just sometimes: when <span class="math inline">\(\p(H) \geq 1/2\)</span>.</li>
<li>Never</li>
</ol></li>
<li><p>Consider this statement:</p>
<ul>
<li>If <span class="math inline">\(\p(E \given H) = 1\)</span> and <span class="math inline">\(0 &lt; \p(E) &lt; 1\)</span>, then <span class="math inline">\(\p(H \given E) &lt; \p(H)\)</span>.</li>
</ul>
<p>Does this statement always hold? If yes, prove that it does. If no, give a counterexample (draw an Euler diagram where the first two conditions hold but not the third).</p></li>
</ol>

</div>
</div>



<p style="text-align: center;">
<a href="multiple-conditions.html"><button class="btn btn-default">Previous</button></a>
<a href="expected-value.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
