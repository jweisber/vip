<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="19 Significance Testing | Odds &amp; Ends" />
<meta property="og:type" content="book" />

<meta property="og:image" content="img/social_image.png" />
<meta property="og:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="github-repo" content="jweisber/vip" />

<meta name="author" content="Jonathan Weisberg" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An open access textbook for introductory philosophy courses on probability and inductive logic.">

<title>19 Significance Testing | Odds &amp; Ends</title>

<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@jweisber" />
<meta name="twitter:creator" content="@jweisber" />
<meta name="twitter:title" content="19 Significance Testing | Odds &amp; Ends" />
<meta name="twitter:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="twitter:image" content="img/social_image.png" />


<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["output/HTML-CSS"],
  "HTML-CSS": {
    availableFonts: ["Gyre-Pagella"],
    preferredFont: "Gyre-Pagella",
    webFont: "Gyre-Pagella",
    imageFont: "Gyre-Pagella"
  }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      extensions: ["color.js"]
    }
  });
</script>




<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="custom.css" type="text/css" />

</head>

<body>


<div style="display: none;">
$$
  \newcommand{\given}{\mid}
  \renewcommand{\neg}{\mathbin{\sim}}
  \renewcommand{\wedge}{\mathbin{\&}}
  \newcommand{\p}{Pr}
  \newcommand{\deg}{^{\circ}}
  \newcommand{\E}{E}
  \newcommand{\EU}{EU}
  \newcommand{\u}{U}
  \newcommand{\pr}{Pr}
  \newcommand{\po}{Pr^*}
  \definecolor{bookred}{RGB}{228,6,19}
  \definecolor{bookblue}{RGB}{0,92,169}
  \definecolor{bookpurple}{RGB}{114,49,94} 
$$
</div>

<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li class="part"><span><b>Part I</b></span></li>
<li class="has-sub"><a href="the-monty-hall-problem.html#the-monty-hall-problem"><span class="toc-section-number">1</span> The Monty Hall Problem</a><ul>
<li><a href="the-monty-hall-problem.html#diagramming-the-solution"><span class="toc-section-number">1.1</span> Diagramming the Solution</a></li>
<li><a href="the-monty-hall-problem.html#lessons"><span class="toc-section-number">1.2</span> Lessons Learned</a></li>
<li><a href="the-monty-hall-problem.html#exercises">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="logic.html#logic"><span class="toc-section-number">2</span> Logic</a><ul>
<li><a href="logic.html#validity-soundness"><span class="toc-section-number">2.1</span> Validity &amp; Soundness</a></li>
<li><a href="logic.html#propositions"><span class="toc-section-number">2.2</span> Propositions</a></li>
<li><a href="logic.html#visualizing-propositions"><span class="toc-section-number">2.3</span> Visualizing Propositions</a></li>
<li><a href="logic.html#strength"><span class="toc-section-number">2.4</span> Strength</a></li>
<li><a href="logic.html#indargs"><span class="toc-section-number">2.5</span> Forms of Inductive Argument</a></li>
<li><a href="logic.html#exercises-1">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="truth-tables.html#truth-tables"><span class="toc-section-number">3</span> Truth Tables</a><ul>
<li><a href="truth-tables.html#connectives"><span class="toc-section-number">3.1</span> Connectives</a></li>
<li><a href="truth-tables.html#truth-tables-1"><span class="toc-section-number">3.2</span> Truth Tables</a></li>
<li><a href="truth-tables.html#logical-truths-contradictions"><span class="toc-section-number">3.3</span> Logical Truths &amp; Contradictions</a></li>
<li><a href="truth-tables.html#mutual-exclusivity-truth-tables"><span class="toc-section-number">3.4</span> Mutual Exclusivity &amp; Truth Tables</a></li>
<li><a href="truth-tables.html#entailment-equivalence"><span class="toc-section-number">3.5</span> Entailment &amp; Equivalence</a></li>
<li><a href="truth-tables.html#summary"><span class="toc-section-number">3.6</span> Summary</a></li>
<li><a href="truth-tables.html#exercises-2">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="the-gamblers-fallacy.html#the-gamblers-fallacy"><span class="toc-section-number">4</span> The Gambler’s Fallacy</a><ul>
<li><a href="the-gamblers-fallacy.html#independence"><span class="toc-section-number">4.1</span> Independence</a></li>
<li><a href="the-gamblers-fallacy.html#fairness"><span class="toc-section-number">4.2</span> Fairness</a></li>
<li><a href="the-gamblers-fallacy.html#the-gamblers-fallacy-1"><span class="toc-section-number">4.3</span> The Gambler’s Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#ignorance-is-not-a-fallacy"><span class="toc-section-number">4.4</span> Ignorance Is Not a Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#the-hot-hand-fallacy"><span class="toc-section-number">4.5</span> The Hot Hand Fallacy</a></li>
<li><a href="the-gamblers-fallacy.html#exercises-3">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="calculating-probabilities.html#calculating-probabilities"><span class="toc-section-number">5</span> Calculating Probabilities</a><ul>
<li><a href="calculating-probabilities.html#multiplying-probabilities"><span class="toc-section-number">5.1</span> Multiplying Probabilities</a></li>
<li><a href="calculating-probabilities.html#adding-probabilities"><span class="toc-section-number">5.2</span> Adding Probabilities</a></li>
<li><a href="calculating-probabilities.html#exclusivity-vs.independence"><span class="toc-section-number">5.3</span> Exclusivity vs. Independence</a></li>
<li><a href="calculating-probabilities.html#tautologies-contradictions-and-equivalent-propositions"><span class="toc-section-number">5.4</span> Tautologies, Contradictions, and Equivalent Propositions</a></li>
<li><a href="calculating-probabilities.html#the-language-of-events"><span class="toc-section-number">5.5</span> The Language of Events</a></li>
<li><a href="calculating-probabilities.html#summary-1"><span class="toc-section-number">5.6</span> Summary</a></li>
<li><a href="calculating-probabilities.html#exercises-4">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="conditional-probability.html#conditional-probability"><span class="toc-section-number">6</span> Conditional Probability</a><ul>
<li><a href="conditional-probability.html#calculating-conditional-probability"><span class="toc-section-number">6.1</span> Calculating Conditional Probability</a></li>
<li><a href="conditional-probability.html#conditional-probability-trees"><span class="toc-section-number">6.2</span> Conditional Probability &amp; Trees</a></li>
<li><a href="conditional-probability.html#more-examples"><span class="toc-section-number">6.3</span> More Examples</a></li>
<li><a href="conditional-probability.html#order-matters"><span class="toc-section-number">6.4</span> Order Matters</a></li>
<li><a href="conditional-probability.html#declaring-independence"><span class="toc-section-number">6.5</span> Declaring Independence</a></li>
<li><a href="conditional-probability.html#ch6ex">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="calculating-probabilities-part-ii.html#calculating-probabilities-part-ii"><span class="toc-section-number">7</span> Calculating Probabilities, Part II</a><ul>
<li><a href="calculating-probabilities-part-ii.html#the-negation-rule"><span class="toc-section-number">7.1</span> The Negation Rule</a></li>
<li><a href="calculating-probabilities-part-ii.html#the-general-addition-rule"><span class="toc-section-number">7.2</span> The General Addition Rule</a></li>
<li><a href="calculating-probabilities-part-ii.html#the-general-multiplication-rule"><span class="toc-section-number">7.3</span> The General Multiplication Rule</a></li>
<li><a href="calculating-probabilities-part-ii.html#laplaces-urn-puzzle"><span class="toc-section-number">7.4</span> Laplace’s Urn Puzzle</a></li>
<li><a href="calculating-probabilities-part-ii.html#the-law-of-total-probability"><span class="toc-section-number">7.5</span> The Law of Total Probability</a></li>
<li><a href="calculating-probabilities-part-ii.html#example"><span class="toc-section-number">7.6</span> Example</a></li>
<li><a href="calculating-probabilities-part-ii.html#exercises-5">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="chbayes.html#chbayes"><span class="toc-section-number">8</span> Bayes’ Theorem</a><ul>
<li><a href="chbayes.html#bayes-theorem"><span class="toc-section-number">8.1</span> Bayes’ Theorem</a></li>
<li><a href="chbayes.html#understanding-bayes-theorem"><span class="toc-section-number">8.2</span> Understanding Bayes’ Theorem</a></li>
<li><a href="chbayes.html#bayes-long-theorem"><span class="toc-section-number">8.3</span> Bayes’ Long Theorem</a></li>
<li><a href="chbayes.html#example-1"><span class="toc-section-number">8.4</span> Example</a></li>
<li><a href="chbayes.html#baserate"><span class="toc-section-number">8.5</span> The Base Rate Fallacy</a></li>
<li><a href="chbayes.html#exercises-6">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="multiple-conditions.html#multiple-conditions"><span class="toc-section-number">9</span> Multiple Conditions</a><ul>
<li><a href="multiple-conditions.html#multiple-draws"><span class="toc-section-number">9.1</span> Multiple Draws</a></li>
<li><a href="multiple-conditions.html#multiple-witnesses"><span class="toc-section-number">9.2</span> Multiple Witnesses</a></li>
<li><a href="multiple-conditions.html#without-replacement"><span class="toc-section-number">9.3</span> Without Replacement</a></li>
<li><a href="multiple-conditions.html#multiplying-conditional-probabilities"><span class="toc-section-number">9.4</span> Multiplying Conditional Probabilities</a></li>
<li><a href="multiple-conditions.html#summary-2"><span class="toc-section-number">9.5</span> Summary</a></li>
<li><a href="multiple-conditions.html#exercises-7">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="probability-induction.html#probability-induction"><span class="toc-section-number">10</span> Probability &amp; Induction</a><ul>
<li><a href="probability-induction.html#generalizing-from-observed-instances"><span class="toc-section-number">10.1</span> Generalizing from Observed Instances</a></li>
<li><a href="probability-induction.html#real-life-is-more-complicated"><span class="toc-section-number">10.2</span> Real Life Is More Complicated</a></li>
<li><a href="probability-induction.html#bayesibe"><span class="toc-section-number">10.3</span> Inference to the Best Explanation</a></li>
</ul></li>
<li class="part"><span><b>Part II</b></span></li>
<li class="has-sub"><a href="expected-value.html#expected-value"><span class="toc-section-number">11</span> Expected Value</a><ul>
<li><a href="expected-value.html#expected-monetary-values"><span class="toc-section-number">11.1</span> Expected Monetary Values</a></li>
<li><a href="expected-value.html#visualizing-expectations"><span class="toc-section-number">11.2</span> Visualizing Expectations</a></li>
<li><a href="expected-value.html#more-than-two-outcomes"><span class="toc-section-number">11.3</span> More Than Two Outcomes</a></li>
<li><a href="expected-value.html#fair-prices"><span class="toc-section-number">11.4</span> Fair Prices</a></li>
<li><a href="expected-value.html#other-goods"><span class="toc-section-number">11.5</span> Other Goods</a></li>
<li><a href="expected-value.html#decision-tables"><span class="toc-section-number">11.6</span> Decision Tables</a></li>
<li><a href="expected-value.html#exercises-8">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="utility.html#utility"><span class="toc-section-number">12</span> Utility</a><ul>
<li><a href="utility.html#subjectivity-objectivity"><span class="toc-section-number">12.1</span> Subjectivity &amp; Objectivity</a></li>
<li><a href="utility.html#the-general-recipe"><span class="toc-section-number">12.2</span> The General Recipe</a></li>
<li><a href="utility.html#choosing-scales"><span class="toc-section-number">12.3</span> Choosing Scales</a></li>
<li><a href="utility.html#a-limitation-the-expected-utility-assumption"><span class="toc-section-number">12.4</span> A Limitation: The Expected Utility Assumption</a></li>
<li><a href="utility.html#the-value-of-money"><span class="toc-section-number">12.5</span> The Value of Money</a></li>
<li><a href="utility.html#exercises-9">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="challenges-to-expected-utility.html#challenges-to-expected-utility"><span class="toc-section-number">13</span> Challenges to Expected Utility</a><ul>
<li><a href="challenges-to-expected-utility.html#the-allais-paradox"><span class="toc-section-number">13.1</span> The Allais Paradox</a></li>
<li><a href="challenges-to-expected-utility.html#the-sure-thing-principle"><span class="toc-section-number">13.2</span> The Sure-thing Principle</a></li>
<li><a href="challenges-to-expected-utility.html#prescriptive-vs.descriptive"><span class="toc-section-number">13.3</span> Prescriptive vs. Descriptive</a></li>
<li><a href="challenges-to-expected-utility.html#the-ellsberg-paradox"><span class="toc-section-number">13.4</span> The Ellsberg Paradox</a></li>
<li><a href="challenges-to-expected-utility.html#ellsberg-allais"><span class="toc-section-number">13.5</span> Ellsberg &amp; Allais</a></li>
<li><a href="challenges-to-expected-utility.html#exercises-10">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="infinity-beyond.html#infinity-beyond"><span class="toc-section-number">14</span> Infinity &amp; Beyond</a><ul>
<li><a href="infinity-beyond.html#the-st.petersburg-paradox"><span class="toc-section-number">14.1</span> The St. Petersburg Paradox</a></li>
<li><a href="infinity-beyond.html#bernoullis-solution"><span class="toc-section-number">14.2</span> Bernoulli’s Solution</a></li>
<li><a href="infinity-beyond.html#st.petersburgs-revenge"><span class="toc-section-number">14.3</span> St. Petersburg’s Revenge</a></li>
<li><a href="infinity-beyond.html#pascals-wager"><span class="toc-section-number">14.4</span> Pascal’s Wager</a></li>
<li><a href="infinity-beyond.html#responses-to-pascals-wager"><span class="toc-section-number">14.5</span> Responses to Pascal’s Wager</a></li>
<li><a href="infinity-beyond.html#exercises-11">Exercises</a></li>
</ul></li>
<li class="part"><span><b>Part III</b></span></li>
<li class="has-sub"><a href="two-schools.html#two-schools"><span class="toc-section-number">15</span> Two Schools</a><ul>
<li><a href="two-schools.html#probability-as-frequency"><span class="toc-section-number">15.1</span> Probability as Frequency</a></li>
<li><a href="two-schools.html#probability-as-belief"><span class="toc-section-number">15.2</span> Probability as Belief</a></li>
<li><a href="two-schools.html#which-kind-of-probability"><span class="toc-section-number">15.3</span> Which Kind of Probability?</a></li>
<li><a href="two-schools.html#frequentism"><span class="toc-section-number">15.4</span> Frequentism</a></li>
<li><a href="two-schools.html#bayesianism"><span class="toc-section-number">15.5</span> Bayesianism</a></li>
</ul></li>
<li class="has-sub"><a href="beliefs-betting-rates.html#beliefs-betting-rates"><span class="toc-section-number">16</span> Beliefs &amp; Betting Rates</a><ul>
<li><a href="beliefs-betting-rates.html#measuring-personal-probabilities"><span class="toc-section-number">16.1</span> Measuring Personal Probabilities</a></li>
<li><a href="beliefs-betting-rates.html#things-to-watch-out-for"><span class="toc-section-number">16.2</span> Things to Watch Out For</a></li>
<li><a href="beliefs-betting-rates.html#indirect-measurements"><span class="toc-section-number">16.3</span> Indirect Measurements</a></li>
<li><a href="beliefs-betting-rates.html#exercises-12">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="dutch-books.html#dutch-books"><span class="toc-section-number">17</span> Dutch Books</a><ul>
<li><a href="dutch-books.html#dutch-books-1"><span class="toc-section-number">17.1</span> Dutch Books</a></li>
<li><a href="dutch-books.html#bankteller"><span class="toc-section-number">17.2</span> The Bankteller Fallacy</a></li>
<li><a href="dutch-books.html#dutch-books-in-general"><span class="toc-section-number">17.3</span> Dutch Books in General</a></li>
<li><a href="dutch-books.html#exercises-13">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="priors.html#priors"><span class="toc-section-number">18</span> The Problem of Priors</a><ul>
<li><a href="priors.html#priors-posteriors"><span class="toc-section-number">18.1</span> Priors &amp; Posteriors</a></li>
<li><a href="priors.html#the-principle-of-indifference"><span class="toc-section-number">18.2</span> The Principle of Indifference</a></li>
<li><a href="priors.html#the-continuous-principle-of-indifference"><span class="toc-section-number">18.3</span> The Continuous Principle of Indifference</a></li>
<li><a href="priors.html#bertrands-paradox"><span class="toc-section-number">18.4</span> Bertrand’s Paradox</a></li>
<li><a href="priors.html#the-problem-of-priors"><span class="toc-section-number">18.5</span> The Problem of Priors</a></li>
<li><a href="priors.html#exercises-14">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="significance-testing.html#significance-testing"><span class="toc-section-number">19</span> Significance Testing</a><ul>
<li><a href="significance-testing.html#coincidence"><span class="toc-section-number">19.1</span> Coincidence</a></li>
<li><a href="significance-testing.html#making-it-precise"><span class="toc-section-number">19.2</span> Making it Precise</a></li>
<li><a href="significance-testing.html#levels-of-significance"><span class="toc-section-number">19.3</span> Levels of Significance</a></li>
<li><a href="significance-testing.html#normal-approximation"><span class="toc-section-number">19.4</span> Normal Approximation</a></li>
<li><a href="significance-testing.html#the-68-95-99-rule"><span class="toc-section-number">19.5</span> The 68-95-99 Rule</a></li>
<li><a href="significance-testing.html#binomial-probabilities"><span class="toc-section-number">19.6</span> Binomial Probabilities</a></li>
<li><a href="significance-testing.html#significance-testing-1"><span class="toc-section-number">19.7</span> Significance Testing</a></li>
<li><a href="significance-testing.html#warnings"><span class="toc-section-number">19.8</span> Warnings</a></li>
<li><a href="significance-testing.html#exercises-15">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="chlindley.html#chlindley"><span class="toc-section-number">20</span> Lindley’s Paradox</a><ul>
<li><a href="chlindley.html#significance-subjectivity"><span class="toc-section-number">20.1</span> Significance &amp; Subjectivity</a></li>
<li><a href="chlindley.html#making-it-concrete"><span class="toc-section-number">20.2</span> Making It Concrete</a></li>
<li><a href="chlindley.html#the-role-of-priors-in-significance-testing"><span class="toc-section-number">20.3</span> The Role of Priors in Significance Testing</a></li>
<li><a href="chlindley.html#lindleys-paradox"><span class="toc-section-number">20.4</span> Lindley’s Paradox</a></li>
<li><a href="chlindley.html#a-bayesian-analysis"><span class="toc-section-number">20.5</span> A Bayesian Analysis</a></li>
<li><a href="chlindley.html#exercises-16">Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="has-sub"><a href="cheat-sheet.html#cheat-sheet"><span class="toc-section-number">A</span> Cheat Sheet</a><ul>
<li><a href="cheat-sheet.html#deductive-logic">Deductive Logic</a></li>
<li><a href="cheat-sheet.html#probability">Probability</a></li>
<li><a href="cheat-sheet.html#decision-theory">Decision Theory</a></li>
<li><a href="cheat-sheet.html#bayesianism-1">Bayesianism</a></li>
<li><a href="cheat-sheet.html#frequentism-1">Frequentism</a></li>
</ul></li>
<li class="has-sub"><a href="the-axioms-of-probability.html#the-axioms-of-probability"><span class="toc-section-number">B</span> The Axioms of Probability</a><ul>
<li><a href="the-axioms-of-probability.html#theories-and-axioms">Theories and Axioms</a></li>
<li><a href="the-axioms-of-probability.html#the-three-axioms-of-probability">The Three Axioms of Probability</a></li>
<li><a href="the-axioms-of-probability.html#first-steps">First Steps</a></li>
<li><a href="the-axioms-of-probability.html#conditional-probability-the-multiplication-rule">Conditional Probability &amp; the Multiplication Rule</a></li>
<li><a href="the-axioms-of-probability.html#equivalence-general-addition">Equivalence &amp; General Addition</a></li>
<li><a href="the-axioms-of-probability.html#total-probability-bayes-theorem">Total Probability &amp; Bayes’ Theorem</a></li>
<li><a href="the-axioms-of-probability.html#independence-1">Independence</a></li>
</ul></li>
<li class="has-sub"><a href="grue.html#grue"><span class="toc-section-number">C</span> The Grue Paradox</a><ul>
<li><a href="grue.html#a-gruesome-concept">A Gruesome Concept</a></li>
<li><a href="grue.html#the-paradox">The Paradox</a></li>
<li><a href="grue.html#grue-artificial-intelligence">Grue &amp; Artificial Intelligence</a></li>
<li><a href="grue.html#disjunctivitis">Disjunctivitis</a></li>
<li><a href="grue.html#time-dependence">Time Dependence</a></li>
<li><a href="grue.html#the-moral">The Moral</a></li>
</ul></li>
<li class="has-sub"><a href="the-problem-of-induction.html#the-problem-of-induction"><span class="toc-section-number">D</span> The Problem of Induction</a><ul>
<li><a href="the-problem-of-induction.html#the-dilemma">The Dilemma</a></li>
<li><a href="the-problem-of-induction.html#the-problem-of-induction-vs.the-grue-paradox">The Problem of Induction vs. the Grue Paradox</a></li>
<li><a href="the-problem-of-induction.html#probability-theory-to-the-rescue">Probability Theory to the Rescue?</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="significance-testing" class="section level1">
<h1><span class="header-section-number">19</span> Significance Testing</h1>
<p><span class="newthought">How</span> do we evaluate a hypothesis, according to frequentism? The short answer is, we look for things that would be too much of a coincidence if the hypothesis were true.</p>
<div id="coincidence" class="section level2">
<h2><span class="header-section-number">19.1</span> Coincidence</h2>
<p><span class="newthought">Suppose</span> we flip a coin ten times and it lands heads every single time. That would be too much of a coincidence if the coin were fair. So the hypothesis that it is fair has been tested, and failed. Conclusion: the coin is biased towards heads.</p>
<p>Or imagine we divide a thousand patients with Disease X into two, equal-sized groups. We give the first group Drug Y, the second group gets a placebo. After a month, <span class="math inline">\(90\%\)</span> of the patients who got the drug are cured, compared to only <span class="math inline">\(10\%\)</span> of the patients who didn’t. That would be too much of a coincidence if the drug were ineffective. So the hypothesis that Drug Y has no effect has been tested, and it has failed the test. Conclusion: Drug Y helps cure Disease X.</p>
<p>That’s the rough idea behind the most popular frequentist approach to hypothesis testing. Now let’s take a closer look.</p>
</div>
<div id="making-it-precise" class="section level2">
<h2><span class="header-section-number">19.2</span> Making it Precise</h2>
<p><span class="newthought">Suppose</span> your friend likes to do magic tricks. Their favourite trick involves flipping a special coin and predicting how it will land. You’re curious how the trick is done, and you suspect the coin is biased. So you flip it <span class="math inline">\(100\)</span> times to see what happens. It lands heads <span class="math inline">\(67\)</span> out of <span class="math inline">\(100\)</span> times. Does that mean the coin is biased after all? Or is <span class="math inline">\(67\)</span> heads out of <span class="math inline">\(100\)</span> flips within the realm of plausibility for a fair coin?</p>
<div class="figure"><span id="fig:binom100"></span>
<p class="caption marginnote shownote">
Figure 19.1: The probability of getting <span class="math inline">\(x\)</span> heads out of <span class="math inline">\(100\)</span> flips of a fair coin.
</p>
<img src="_main_files/figure-html/binom100-1.png" alt="The probability of getting $x$ heads out of $100$ flips of a fair coin." width="672"  />
</div>
<p>Figure <a href="significance-testing.html#fig:binom100">19.1</a> shows the probability for each number of heads we might get, if the coin is fair. Notice how unlikely <span class="math inline">\(67\)</span> heads is, compared to say <span class="math inline">\(50\)</span> heads, or even compared to <span class="math inline">\(60\)</span> heads. So it seems like a stretch to just write off our <span class="math inline">\(67\)</span> heads as a coincidence. Plausibly, the coin is biased towards heads.</p>
<p>What if we’d gotten only <span class="math inline">\(65\)</span> heads though? Or just <span class="math inline">\(60\)</span>? At what point do we just say, “oh well, that’s a normal outcome for a fair coin”?</p>
<p>A common convention is to draw the line at <span class="math inline">\(95\%\)</span> probability. If a coin is fair, then <span class="math inline">\(95\%\)</span> of the time it will land heads between <span class="math inline">\(40\)</span> and <span class="math inline">\(60\)</span> times out of <span class="math inline">\(100\)</span> tosses. That’s not obvious by the way: I used a computer to find that <span class="math inline">\(40\)</span>-to-<span class="math inline">\(60\)</span> range. But we’ll come back to these calculations in a bit.</p>
<div class="figure"><span id="fig:binom100fences"></span>
<p class="caption marginnote shownote">
Figure 19.2: In <span class="math inline">\(95\%\)</span> of cases, a fair coin will land heads between <span class="math inline">\(40\)</span> and <span class="math inline">\(60\)</span> times out of <span class="math inline">\(100\)</span> flips. If the number of heads falls outside that range, we conclude the coin is not fair.
</p>
<img src="_main_files/figure-html/binom100fences-1.png" alt="In $95\%$ of cases, a fair coin will land heads between $40$ and $60$ times out of $100$ flips. If the number of heads falls outside that range, we conclude the coin is not fair." width="672"  />
</div>
<p>The key idea for now is this. The hypothesis the coin is fair fails the test if the number of heads falls <em>outside</em> the <span class="math inline">\(40\)</span>-to-<span class="math inline">\(60\)</span> range. As Figure <a href="significance-testing.html#fig:binom100fences">19.2</a> illustrates, we’ll reject the hypothesis if the outcome of our experiment is one of the red ones.</p>
<p>We call the result of an experiment <strong><em>statistically significant</em></strong> when it falls outside the <span class="math inline">\(95\%\)</span> range. So the red outcomes in Figure <a href="significance-testing.html#fig:binom100fences">19.2</a> are statistically significant, the blue ones are not.</p>
<p>The idea behind this terminology is that a red outcome tells us something about our hypothesis that the coin is fair. In particular it tells us something negative: the hypothesis is not looking good. Because <span class="math inline">\(67\)</span> heads would be a big coincidence if the hypothesis were true.</p>
</div>
<div id="levels-of-significance" class="section level2">
<h2><span class="header-section-number">19.3</span> Levels of Significance</h2>
<p><span class="newthought">But</span> why should we choose <span class="math inline">\(95\%\)</span> probability as our cutoff? Why not <span class="math inline">\(90\%\)</span>, or <span class="math inline">\(99\%\)</span>?</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:binom100fences99"></span>
<img src="_main_files/figure-html/binom100fences99-1.png" alt="In $99\%$ of cases, a fair coin will land heads between $37$ and $63$ times out of $100$ flips." width="672"  />
<!--
<p class="caption marginnote">-->Figure 19.3: In <span class="math inline">\(99\%\)</span> of cases, a fair coin will land heads between <span class="math inline">\(37\)</span> and <span class="math inline">\(63\)</span> times out of <span class="math inline">\(100\)</span> flips.<!--</p>-->
<!--</div>--></span>
</p>
<p>In fact we don’t have to make <span class="math inline">\(95\%\)</span> the cutoff. In some sciences it’s customary to make <span class="math inline">\(99\%\)</span> the cutoff instead, or even <span class="math inline">\(99.9\%\)</span>.</p>
<p>To be explicit about what cutoff we’re using, we describe the result as statistically significant <em>at such-and-such a level</em>. For example, if we’re using the <span class="math inline">\(95\%\)</span> cutoff, we say the result is <strong><em>significant at the <span class="math inline">\(0.05\)</span> level</em></strong>. And if we’re using the <span class="math inline">\(99\%\)</span> cutoff, we say the result is <strong><em>significant at the <span class="math inline">\(0.01\)</span> level</em></strong>. And so on.</p>
<p>Different sciences have different conventions about where to put the cutoff. Social sciences like Psychology typically use the <span class="math inline">\(95\%\)</span> cutoff. Only when the outcome of a study is significant at the <span class="math inline">\(.05\)</span> level is the hypothesis rejected. But medical and physical sciences often use a stricter cutoff like <span class="math inline">\(99\%\)</span>. A finding has to be significant at the <span class="math inline">\(.01\)</span> level to disprove a hypothesis then.</p>
<p>There’s nothing special or magical about the <span class="math inline">\(95\%\)</span> and <span class="math inline">\(99\%\)</span> cutoffs, though. So why have scientists adopted these conventions? This is a deep and important question, with no easy answer.</p>
<p>But part of the answer is that it’s actually just kind of a historical and mathematical accident. Before computers, it was hard to calculate significance levels exactly. There’s a trick though for estimating the <span class="math inline">\(95\%\)</span> and <span class="math inline">\(99\%\)</span> cutoffs, which we’ll learn in the next section. So scientists adopted these conventions before computers came along, partly just because they were easy to work with. And now they’ve kind of just stuck.</p>
<p>We’ll come back in <a href="chlindley.html#chlindley">the next chapter</a> for a deeper look at the question of where to put the cutoff for statistical significance. But, for now, let’s learn a quick and easy way of finding the <span class="math inline">\(95\%\)</span> and <span class="math inline">\(99\%\)</span> cutoffs.</p>
</div>
<div id="normal-approximation" class="section level2">
<h2><span class="header-section-number">19.4</span> Normal Approximation</h2>
<p><span class="newthought">You</span> may have noticed that the probabilities in our hundred-flips experiment look a lot like the famous “bell curve”. In fact they line up almost perfectly, as Figure <a href="significance-testing.html#fig:napprox">19.4</a> illustrates.</p>
<div class="figure"><span id="fig:napprox"></span>
<p class="caption marginnote shownote">
Figure 19.4: A bell curve (left) overlayed on the probability of getting <span class="math inline">\(x\)</span> heads out of <span class="math inline">\(100\)</span> flips of a fair coin (right).
</p>
<img src="_main_files/figure-html/napprox-1.png" alt="A bell curve (left) overlayed on the probability of getting $x$ heads out of $100$ flips of a fair coin (right)." width="672"  />
</div>
<p><label for="tufte-mn-29" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-29" class="margin-toggle"><span class="marginnote"><span style="display: block;">The normal distribution actually has a very complicated mathematical formula. You <em>really</em> don’t need to know it for this book, but if you’re curious you can find it <a href="https://en.wikipedia.org/wiki/Normal_distribution">on Wikipedia</a>.</span></span></p>
<p>The bell curve’s official name is the <strong><em>normal distribution</em></strong>. And it has some very handy mathematical properties, which make it easy to estimate when the number of heads falls outside the <span class="math inline">\(95\%\)</span> range. Two features of the normal distribution are required for the calculation.</p>
<p><label for="tufte-mn-30" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-30" class="margin-toggle"><span class="marginnote"><span style="display: block;">The symbol <span class="math inline"><span class="math inline">\(\mu\)</span></span> is from Greek and is called <em>mu</em> (rhymes with <em>stew</em>). It looks like the English letter <em>u</em>, but it actually corresponds to the letter <em>m</em>, as in <em>m</em>ean. It helps to picture it as a cursive <em>m</em>, written very sloppily by someone in a hurry.</span></span></p>
<p>First we need to know where the bell is located: where is it centred? This is called the <strong><em>mean</em></strong>, or <span class="math inline">\(\mu\)</span> for short. In our example <span class="math inline">\(\mu = 50\)</span>. That’s the most likely outcome for <span class="math inline">\(100\)</span> flips of a fair coin. The general formula is
<span class="math display">\[ \mu = np, \]</span>
where <span class="math inline">\(n\)</span> is the number of tosses, and <span class="math inline">\(p\)</span> is the probability of heads on each toss. So in our example <span class="math inline">\(np = (100)(1/2) = 50\)</span>.</p>
<p><label for="tufte-mn-31" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-31" class="margin-toggle"><span class="marginnote"><span style="display: block;">The Greek symbol <span class="math inline"><span class="math inline">\(\sigma\)</span></span> is called <em>sigma</em>. It corresponds to the English letter <em>s</em>, as in <em>s</em>tandard deviation.</span></span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:normalsds"></span>
<img src="_main_files/figure-html/normalsds-1.png" alt="Four bell curves with the same mean of $50$, but different standard deviations. The larger the standard deviation, the wider the bell." width="672"  />
<!--
<p class="caption marginnote">-->Figure 19.5: Four bell curves with the same mean of <span class="math inline">\(50\)</span>, but different standard deviations. The larger the standard deviation, the wider the bell.<!--</p>-->
<!--</div>--></span>
</p>
<p>Second, how wide is the bell? This is called the <strong><em>standard deviation</em></strong>, or <span class="math inline">\(\sigma\)</span> for short. The formula for <span class="math inline">\(\sigma\)</span> is a bit mysterious:
<span class="math display">\[ \sigma = \sqrt{np(1-p)}. \]</span>
Deriving this formula is pretty advanced, so we’ll have to just take it on faith. We only need to understand that the larger the standard deviation, the wider the bell. Figure <a href="significance-testing.html#fig:normalsds">19.5</a> gives some examples to illustrate.</p>
<p>Applying the formula for standard deviation to our example, we get:
<span class="math display">\[
  \begin{aligned}
    \sigma &amp;= \sqrt{(100)(1/2)(1/2)}\\
           &amp;= 5.
  \end{aligned}
\]</span></p>
<p>Now for the punchline: we can use the numbers <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> to get a pretty accurate estimate of the <span class="math inline">\(95\%\)</span> cutoff.</p>
<p>Mathematicians have proved that about <span class="math inline">\(95\%\)</span> of the time, the number of heads will be within the range <span class="math inline">\(\mu \pm 2\sigma\)</span>. In our example <span class="math inline">\(2\sigma = (2)(5) = 10\)</span>, so <span class="math inline">\(\mu \pm 2\sigma\)</span> is the range from <span class="math inline">\(40\)</span> to <span class="math inline">\(60\)</span>. Notice how this is the same as the computer-based answer we used earlier.</p>
<p>For a <span class="math inline">\(99\%\)</span> cutoff we just multiply <span class="math inline">\(\sigma\)</span> by <span class="math inline">\(3\)</span> instead of <span class="math inline">\(2\)</span>. In other words, <span class="math inline">\(99\%\)</span> of the time the number of heads will be within the range <span class="math inline">\(\mu \pm 3\sigma\)</span>. In our example <span class="math inline">\(3\sigma = (3)(5) = 15\)</span>, so <span class="math inline">\(\mu \pm 3\sigma\)</span> is the range from <span class="math inline">\(35\)</span> to <span class="math inline">\(65\)</span>.</p>
<p>We got <span class="math inline">\(67\)</span> heads in our experiment, which falls outside the <span class="math inline">\(35\)</span>-to-<span class="math inline">\(65\)</span> range. So our result wasn’t just significant at the <span class="math inline">\(.05\)</span> level. It was significant at the <span class="math inline">\(.01\)</span> level too.</p>
</div>
<div id="the-68-95-99-rule" class="section level2">
<h2><span class="header-section-number">19.5</span> The 68-95-99 Rule</h2>
<p><span class="newthought">There</span> are actually three cutoffs we can estimate with this method. The one we haven’t talked about is <span class="math inline">\(68\%\)</span>, because it’s not used much in actual practice. But the general rule is:</p>
<ul>
<li>With about <span class="math inline">\(68\%\)</span> probability, the number of heads will be in the range <span class="math inline">\(\mu \pm \sigma\)</span>.</li>
<li>With about <span class="math inline">\(95\%\)</span> probability, the number of heads will be in the range <span class="math inline">\(\mu \pm 2\sigma\)</span>.</li>
<li>With about <span class="math inline">\(99\%\)</span> probability, the number of heads will be in the range <span class="math inline">\(\mu \pm 3\sigma\)</span>.</li>
</ul>
<p>This is called <strong><em>The 68-95-99 Rule</em></strong>. We can illustrate it with a diagram like Figure <a href="significance-testing.html#fig:snnrule">19.6</a>.</p>
<div class="figure"><span id="fig:snnrule"></span>
<p class="caption marginnote shownote">
Figure 19.6: The <span class="math inline">\(68\)</span>-<span class="math inline">\(95\)</span>-<span class="math inline">\(99\)</span> Rule
</p>
<img src="_main_files/figure-html/snnrule-11.png" alt="The $68$-$95$-$99$ Rule" width="672"  />
</div>
<p>It’s customary to use <span class="math inline">\(k\)</span> for the number of heads we actually get in our experiment. So we can also state the rule formally like this:
<span class="math display">\[
  \begin{aligned}
    \p(\mu - \sigma \leq k \leq \mu + \sigma) &amp;\approx .68,\\
    \p(\mu - 2\sigma \leq k \leq \mu + 2\sigma) &amp;\approx .95,\\
    \p(\mu - 3\sigma \leq k \leq \mu + 3\sigma) &amp;\approx .99.
  \end{aligned}
\]</span></p>
</div>
<div id="binomial-probabilities" class="section level2">
<h2><span class="header-section-number">19.6</span> Binomial Probabilities</h2>
<p><span class="newthought">When</span> do the true probabilities closely match the normal distribution, though? A lot of the time, actually. But we’ll just focus on one common case.</p>
<p>Suppose a certain kind of event will be repeated over and over, and there are two possible outcomes. If the probabilities of these two outcomes stay the same from repetition to repetition, and the repetitions are independent of one another, then the probabilities are called <strong><em>binomial</em></strong>.</p>
<p>Binomial probabilities are very common. We saw that they apply to flips of a fair coin. Another example could be patients in a drug study who either get better or don’t. Or subjects in a psychology study answering yes/no questions on a survey. Or respondents to a poll about a political race.</p>
<p>If the event is repeated enough times, the binomial probabilities will closely match the bell curve. But you have to be careful: the more extreme the probability in your hypothesis, the larger the study will need to be. If the probability <span class="math inline">\(p\)</span> is close to either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, the normal approximation won’t be very good without a pretty large number of trials <span class="math inline">\(n\)</span>.</p>
<p>Suppose for example your hypothesis is that the coin is heavily biased towards tails: <span class="math inline">\(p = 0.05\)</span>, to be exact. Then we see from Figure <a href="significance-testing.html#fig:npgrid">19.7</a> that we need a sample of around <span class="math inline">\(n = 50\)</span> before the normal approximation starts to become reasonably accurate. Whereas <span class="math inline">\(n = 30\)</span> is already quite accurate for <span class="math inline">\(p = 0.5\)</span>.</p>
<div class="figure fullwidth"><span id="fig:npgrid"></span>
<img src="_main_files/figure-html/npgrid-1.png" alt="When $p$ is close to $0$ or $1$, we need a larger $n$ to make the normal approximation accurate." width="1152"  />
<p class="caption marginnote shownote">
Figure 19.7: When <span class="math inline">\(p\)</span> is close to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, we need a larger <span class="math inline">\(n\)</span> to make the normal approximation accurate.
</p>
</div>
<p>In general, the more extreme the value of <span class="math inline">\(p\)</span> is, the larger we need <span class="math inline">\(n\)</span> to be for the approximation to be accurate. But if you do a big enough study, the <span class="math inline">\(68\)</span>-<span class="math inline">\(95\)</span>-<span class="math inline">\(99\)</span> rule will give a reliable estimate.</p>
</div>
<div id="significance-testing-1" class="section level2">
<h2><span class="header-section-number">19.7</span> Significance Testing</h2>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:unnamed-chunk-143"></span>
<img src="img/fisher.png" alt="Ronald A. Fisher (1890--1962) established significance testing as a standard scientific method with his influential $1925$ book *Statistical Methods for Research Workers*." width="134"  />
<!--
<p class="caption marginnote">-->Figure 19.8: Ronald A. Fisher (1890–1962) established significance testing as a standard scientific method with his influential <span class="math inline">\(1925\)</span> book <em>Statistical Methods for Research Workers</em>.<!--</p>-->
<!--</div>--></span>
</p>
<p><span class="newthought">The</span> method of evaluating hypotheses we’ve been explaining is called <strong><em>significance testing</em></strong>. Here is the general recipe for significance testing with binomial probabilities:</p>
<ol style="list-style-type: decimal">
<li>State the hypothesis you want to test: the true probability of outcome X is <span class="math inline">\(p\)</span>. This is called the <strong><em>null hypothesis</em></strong>.<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> Why is it called the “null” hypothesis? The idea is that our default assumption should be that things are random, until we discover a pattern. For example, we assume our sequence of heads/tails is random unless our observations prove otherwise. And we assume a drug has no effects unless we discover that it does. So the default assumption is “null”: no pattern, no effect. It’s not a great piece of terminology, but unfortunately we’re stuck with it.</span></li>
<li>Repeat the event over and over <span class="math inline">\(n\)</span> times, and count the number of times <span class="math inline">\(k\)</span> you get outcome X.</li>
<li>Calculate <span class="math inline">\(\mu = np\)</span> and <span class="math inline">\(\sigma = \sqrt{np(1-p)}\)</span>.</li>
<li>Use the <span class="math inline">\(68\)</span>-<span class="math inline">\(95\)</span>-<span class="math inline">\(99\)</span> rule to figure out how much of a coincidence your finding <span class="math inline">\(k\)</span> must be if the null hypothesis is true.</li>
<li>If it’s too much of a coincidence, conclude that the null hypothesis is false. The true probability isn’t <span class="math inline">\(p\)</span>.</li>
</ol>
<p><span class="newthought">Let’s do one</span> more example for practice.</p>
<p><label for="tufte-mn-32" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-32" class="margin-toggle"><span class="marginnote"><span style="display: block;">This example is from page <span class="math inline"><span class="math inline">\(205\)</span></span> of Ian Hacking’s excellent textbook, <em>An Introduction to Probability &amp; Inductive Logic</em>.</span></span></p>
<div class="example">
<p>
A company named VisioPerfect makes “long-life” lightbulbs. According to their ads, <span class="math inline"><span class="math inline">\(96\%\)</span></span> of their bulbs outlast their competitors’ average lifespan.
</p>
<p>
The magazine <em>Consumers’ Advocate</em> decides to run a test of 2,400 VisioPerfect bulbs. They find that <span class="math inline"><span class="math inline">\(133\)</span></span> of them weren’t “long-life”.<br />
</p>
</div>
<p>Is the result of this test bad news for VisioPerfect? Are their ads just hype? Let’s test their claim.</p>
<p>The hypothesis we’re testing is that each bulb has probability <span class="math inline">\(.96\)</span> of being long-life, which means it has probability <span class="math inline">\(p = .04\)</span> of being “short-life”. The magazine found <span class="math inline">\(k = 133\)</span> short-life bulbs out of <span class="math inline">\(n = 2,400\)</span>. Is this about what you’d expect if the ads are honest? Let’s start by finding the center and width of our normal approximation:
<span class="math display">\[
  \begin{aligned}
    \mu &amp;= np = (2,400)(.04) = 96,\\
    \sigma &amp;= \sqrt{np(1-p)} = \sqrt{(2,400)(.04)(.96)} = 9.6.
  \end{aligned}
\]</span>
Then we use the <span class="math inline">\(68\)</span>-<span class="math inline">\(95\)</span>-<span class="math inline">\(99\)</span> rule to figure out where <span class="math inline">\(k = 133\)</span> falls. In this case the <span class="math inline">\(99\%\)</span> range is <span class="math inline">\(96 \pm (3)(9.6)\)</span>, which is <span class="math inline">\(67.2\)</span> to <span class="math inline">\(124.8\)</span>. Since <span class="math inline">\(k = 133\)</span> falls outside this range, it’s a pretty far out result. It’s the kind of thing you’d expect to happen less than <span class="math inline">\(1\%\)</span> of the time if each bulb really had a <span class="math inline">\(96\%\)</span> chance of being long-life.</p>
<p>So VisioPerfect’s claim has failed our test: the results are significant at the <span class="math inline">\(.01\)</span> level.</p>
</div>
<div id="warnings" class="section level2">
<h2><span class="header-section-number">19.8</span> Warnings</h2>
<p><label for="tufte-mn-33" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-33" class="margin-toggle"><span class="marginnote"><span style="display: block;">The American Statistical Association <a href="https://www.amstat.org/newsroom/pressreleases/P-ValueStatement.pdf">recently released a statement</a> to clarify the ideas behind significance testing, and prevent their misuse. It’s also the punchline of <a href="https://imgs.xkcd.com/comics/significant.png">one of my favourite cartoons</a>.</span></span></p>
<p><span class="newthought">Significance</span> testing is very confusing. So confusing that scientists, and even statisticians, often misunderstand and misuse it. Dangerous medical treatments have been approved and administered as a result. Scientific careers have even been built on the misuse of significance testing.</p>
<p>So what exactly does it mean for a result to be “significant at level <span class="math inline">\(\alpha\)</span>”? It means exactly this:</p>
<p><label for="tufte-mn-34" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-34" class="margin-toggle"><span class="marginnote"><span style="display: block;">The Greek letter <span class="math inline"><span class="math inline">\(\alpha\)</span></span> is called <em>alpha</em> and corresponds to the English letter <em>a</em>. It’s customarily used for significance levels, I don’t know why.</span></span></p>
<div class="warning">
<p>
<em>If</em> the hypothesis we are testing is true, <em>then</em> a result this unusual was less than <span class="math inline"><span class="math inline">\(\alpha\)</span></span> likely to occur.
</p>
</div>
<p>Notice how this is an <em>if/then</em> statement, where the <em>if</em> part supposes that the hypothesis we’re testing is true. We’re considering what things look like from the hypothetical point of view where the hypothesis is true. Then we assess how probable various outcomes are, given that assumption.</p>
<p><label for="tufte-mn-35" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-35" class="margin-toggle"><span class="marginnote"><span style="display: block;">“What’s wrong with NHST [null hypothesis significance testing]? Well, among many other things, it does not tell us what we want to know…” —Jacob Cohen</span></span></p>
<p>But notice, what we really want to know is the reverse thing. We want to know: <em>if</em> we get <span class="math inline">\(k\)</span> heads, <em>then</em> how likely is it the hypothesis is true? You might think if the result is significant at the <span class="math inline">\(.05\)</span> level, then the hypothesis is less than <span class="math inline">\(.05\)</span> probable. Not so!</p>
<div class="warning">
<p>
Just because the outcome of an experiment is significant at the <span class="math inline"><span class="math inline">\(\alpha\)</span></span> level doesn’t mean the probability of the hypothesis is <span class="math inline"><span class="math inline">\(\alpha\)</span></span> (or less than <span class="math inline"><span class="math inline">\(\alpha\)</span></span>).
</p>
</div>
<p>We learned back in <a href="chbayes.html#bayes-theorem">Chapter 8</a> about how <span class="math inline">\(\p(H \given E)\)</span> and <span class="math inline">\(\p(E \given H)\)</span> are different things. In fact they can be very different; one can be high and the other low. The <a href="chbayes.html#fig:taxigrid">taxicab problem</a> is one famous illustration. And it’s a very common mistake to make a similar confusion with significance testing.</p>
<p>We’ll deepen our understanding of this point in <a href="chlindley.html#chlindley">the next chapter</a>.</p>
</div>
<div id="exercises-15" class="section level2 unnumbered">
<h2>Exercises</h2>
<ol>
<li><p>Suppose <span class="math inline">\(20\%\)</span> of the marbles in an urn are green. We are going to randomly draw <span class="math inline">\(100\)</span> marbles, with replacement, and then count the number of draws that are green.</p>
<p>Let’s use a normal distribution to approximate the probability of getting <span class="math inline">\(k\)</span> green balls.</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation?</li>
<li>Draw a rough graph of the corresponding normal curve.</li>
<li>The probability is about <span class="math inline">\(.68\)</span> that the number of green draws will fall between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>?</li>
<li>What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for an approximate probability of <span class="math inline">\(.95\)</span>?</li>
<li>What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for an approximate probability of <span class="math inline">\(.99\)</span>?</li>
<li>If you get fewer than the expected <span class="math inline">\(20\)</span> greens, how few do you have to get for the results to be significant at the <span class="math inline">\(.01\)</span> level? (Assume the estimates above are accurate.)</li>
<li>If you get more than the expected <span class="math inline">\(20\)</span> greens, how many do you have to get for the results to be significant at the <span class="math inline">\(.01\)</span> level? (Assume the estimates above are accurate.)</li>
<li>If you got one of these “significant” results in this experiment, would you reject the hypothesis that the urn contains <span class="math inline">\(20\%\)</span> green marbles? (This is a question about your personal judgment.)</li>
</ol></li>
<li><p>Suppose <span class="math inline">\(60\%\)</span> of the marbles in an urn are green, and we are going to randomly draw <span class="math inline">\(150\)</span> marbles, with replacement.</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation?</li>
<li>Draw a rough graph of the corresponding normal curve.</li>
<li>The probability is about <span class="math inline">\(.95\)</span> that the number of green draws will fall between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>?</li>
<li>What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for an approximate probability of <span class="math inline">\(.99\)</span>?</li>
<li>If you do the <span class="math inline">\(150\)</span> draws and you only get <span class="math inline">\(80\)</span> greens, is that significant at the <span class="math inline">\(.05\)</span> level? (Assume the estimates above are accurate.)</li>
<li>If instead you do the <span class="math inline">\(150\)</span> draws and you only get <span class="math inline">\(70\)</span> greens, is that significant at the <span class="math inline">\(.05\)</span> level? (Assume the estimates above are accurate.)</li>
<li>Would you reject the hypothesis that the urn contains <span class="math inline">\(60\%\)</span> green marbles if you only got 70 green draws?</li>
</ol></li>
<li><p>Dr. Colbert claims to have a miraculous new weight-loss product. According to him, <span class="math inline">\(75\%\)</span> percent of people who use it lose weight. But the Ministry of Health is suspicious so they run a study. They recruit <span class="math inline">\(192\)</span> subjects to try Dr. Colbert’s new treatment. <span class="math inline">\(135\)</span> of them lose weight.</p>
<p>Use a normal approximation to assess Dr. Colbert’s claim:</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation?</li>
<li>Fill in the blanks: the results of the study are significant at the <span class="math inline">\(.05\)</span> level if the number of subjects who lose weight is either smaller than _____ or greater than _____.</li>
<li>Fill in the blanks: the results of the study are significant at the <span class="math inline">\(.01\)</span> level if the number of subjects who lose weight is either smaller than _____ or greater than _____.</li>
<li>Are the results of the Health Canada study significant at the <span class="math inline">\(.05\)</span> level? Are they significant at the <span class="math inline">\(.01\)</span> level?</li>
<li>True or false: given the results of the study, the probability that Dr. Colbert’s product works as advertised is less than <span class="math inline">\(.05\)</span>.</li>
</ol></li>
<li><p>Your professor says <span class="math inline">\(80\%\)</span> of the class passed the midterm, but that seems high since the test was so hard. So you ask all the people sitting in your row if they passed: <span class="math inline">\(4\)</span> of them did, <span class="math inline">\(3\)</span> didn’t. Since you didn’t pass either, that’s a pass-rate of only <span class="math inline">\(50\%\)</span> in your sample.</p>
<p>Use a normal approximation to answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>Is your finding significant at the <span class="math inline">\(.05\)</span> level?</li>
<li>Is it significant at the <span class="math inline">\(.01\)</span> level?</li>
<li>True or false: given your findings, the probability that your professor’s claim is true is at least <span class="math inline">\(.05\)</span>.</li>
</ol></li>
<li><p>Suppose you arrange seven dates next week, one for each night of the week. You ask each date the same question: which was your favourite grade, out of grades <span class="math inline">\(1\)</span> through <span class="math inline">\(8\)</span>? Your first and last dates both give the same answer as you: [insert your favourite grade here]. Is this result significant? Check both the <span class="math inline">\(.05\)</span> and <span class="math inline">\(.01\)</span> levels using a normal approximation.</p>
<p>As a null hypothesis, assume that each person’s answer is independent (including yours), and that each person is equally likely to name any one grade as any other (including you).</p></li>
<li><p>Suppose <span class="math inline">\(10\%\)</span> of all wine bottles have corks (instead of screw-tops). A restaurant opens <span class="math inline">\(400\)</span> random bottles in a month and counts the number that are corked.</p>
<p>Use a normal distribution to approximate the probability that the restaurant will open <span class="math inline">\(k\)</span> corked bottles.</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation?</li>
<li>The probability is about <span class="math inline">\(.68\)</span> that the number of corked bottles will fall between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>?</li>
<li>What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for an approximate probability of <span class="math inline">\(.95\)</span>?</li>
<li>What are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for an approximate probability of <span class="math inline">\(.99\)</span>?</li>
</ol>
<p>The restaurant wants the head waiter to avoid serving corked bottles since they’re more expensive. So they offer her a bonus if fewer than the expected number of corked bottles gets opened, provided the results are significant at the <span class="math inline">\(.01\)</span> level. On the other hand they’ll fire her if more than the expected number of corked bottles gets opened, assuming the results are significant at the <span class="math inline">\(.01\)</span> level.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li>How few corked bottles have to be opened for the head waiter to get the bonus? How many corked bottles have to opened for the head waiter to lose her job? (Assume the estimates above are accurate.)</li>
</ol></li>
<li><p>A few years ago Arkansas passed a law sparking a debate about a new treatment. The treatment is supposed to help a person’s pregnancy continue under conditions that usually end it.</p>
<p>Normally under those conditions, only about <span class="math inline">\(40\%\)</span> of people’s pregnancies continue. But in a small case study, six pregnant people were given the new treatment, and four of them continued their pregnancies.</p>
<p>Critics say this result is not significant: it doesn’t show the treatment has an effect. (If you’re curious, you can read more about the controversy
<a href="http://www.theatlantic.com/health/archive/2015/03/abortion-reversal-arizona/388880/">here</a>.)</p>
<p>Use a normal approximation to assess this criticism. The null hypothesis is that the treatment has no effect, so each of the six pregnancies has a <span class="math inline">\(0.4\)</span> chance of continuing.</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the case study?</li>
<li>Are the results of the case study significant at the <span class="math inline">\(.05\)</span> level? (Assume the normal approximation is accurate.)</li>
<li>Suppose a much larger study were done, with <span class="math inline">\(150\)</span> subjects getting the treatment, and two-thirds of them continuing their pregnancies. What would <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> be then? Would the results be significant at the <span class="math inline">\(.01\)</span> level?</li>
</ol></li>
<li><p>Medical researchers are testing a new cancer treatment. Ordinarily, a patient’s chance of going into remission is only <span class="math inline">\(1/10\)</span>. The null hypothesis is that the drug will have no effect on patients’ chances of going into remission.</p>
<p>They select <span class="math inline">\(100\)</span> patients at random and give each one the new treatment. The result: <span class="math inline">\(18\)</span> of them go into remission.</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in a normal approximation here?</li>
<li>According to the normal approximation, is the result significant at the <span class="math inline">\(.01\)</span> level?</li>
<li>True or false: the fact that the result is significant at the <span class="math inline">\(.05\)</span> level tells us that the null hypothesis is less than <span class="math inline">\(.05\)</span> probable.</li>
<li>True or false: the fact that the result is significant at the <span class="math inline">\(.05\)</span> level tells us that, if the null hypothesis is true, then the result was <span class="math inline">\(.05\)</span> probable.</li>
</ol></li>
<li><p>You’ve seen Gonzo the Great doing magic tricks with a coin. You suspect he’s using a biased coin, so you sneak into his dressing room and steal the coin.</p>
<p>Your hypothesis is that the coin has a <span class="math inline">\(8/10\)</span> bias towards heads. You flip it <span class="math inline">\(100\)</span> times and it comes up heads <span class="math inline">\(65\)</span> times.</p>
<ol style="list-style-type: lower-alpha">
<li>What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation here?</li>
<li>According to the normal approximation, is the result significant at the <span class="math inline">\(.01\)</span> level?</li>
<li>Explain, in terms of frequencies, what it means for a result to be “significant at the <span class="math inline">\(.01\)</span> level”.</li>
</ol></li>
<li><p>Suppose the registrar has a list of all the student numbers from a class, but they’ve lost the data that says which class it is. They know it’s either Philosophy <span class="math inline">\(101\)</span> or Economics <span class="math inline">\(101\)</span>. But they have no idea which of those two it is: the two hypotheses are equally probable, <span class="math inline">\(50\%\)</span>.</p>
<p>If it’s Philosophy <span class="math inline">\(101\)</span>, then <span class="math inline">\(40\%\)</span> of the students are philosophy majors. If it’s Economics <span class="math inline">\(101\)</span>, then only <span class="math inline">\(25\%\)</span> of the class are philosophy majors.</p>
<p>The registrar picks ten student numbers from the list at random and looks up the students’ majors. The result: all ten of them are philosophy majors.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose our null hypothesis is that the list is for Philosophy <span class="math inline">\(101\)</span>. What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation? (You may use a decimal approximation here.)</li>
<li>Is the result significant at the <span class="math inline">\(.01\)</span> level for this null hypothesis?</li>
<li>Suppose our null hypothesis is that the list is for Economics <span class="math inline">\(101\)</span>. What are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in the normal approximation then? (You may use a decimal approximation here.)</li>
<li>Is the result significant at the <span class="math inline">\(.01\)</span> level for this null hypothesis?</li>
</ol></li>
<li><p>Explain in your own words what it means for a result to be significant at the <span class="math inline">\(.05\)</span> level. See if you can do it without looking back over the text of this chapter.</p></li>
</ol>

</div>
</div>
<p style="text-align: center;">
<a href="priors.html"><button class="btn btn-default">Previous</button></a>
<a href="chlindley.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
